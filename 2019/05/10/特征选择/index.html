<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>特征选择 | Moby Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="whatfeature-selector是一个基础的特征选择工具，主要对以下类型的特征进行选择：  高missing-values 强相关性的特征 对模型预测结果无贡献的或者很小贡献的 特征值只有一个单独变量  通常拿到一个数据集的时候，先用feature-selector筛一遍，主要提供以下五个函数： 12345identify_missing(*)identify_collinear(*)id">
<meta name="keywords" content="数据分析，数据挖掘">
<meta property="og:type" content="article">
<meta property="og:title" content="特征选择">
<meta property="og:url" content="http://mobyIsMe.github.io/2019/05/10/特征选择/index.html">
<meta property="og:site_name" content="Moby Blog">
<meta property="og:description" content="whatfeature-selector是一个基础的特征选择工具，主要对以下类型的特征进行选择：  高missing-values 强相关性的特征 对模型预测结果无贡献的或者很小贡献的 特征值只有一个单独变量  通常拿到一个数据集的时候，先用feature-selector筛一遍，主要提供以下五个函数： 12345identify_missing(*)identify_collinear(*)id">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://user-images.githubusercontent.com/9191594/60969408-07803800-a352-11e9-9294-cdc1c17b0ae2.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/9191594/60969486-35657c80-a352-11e9-966d-f920717d184c.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/9191594/60969615-8bd2bb00-a352-11e9-916f-d0d33bd292f4.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/9191594/60969644-a016b800-a352-11e9-92b7-3927dce6b1ca.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/9191594/60969744-dfdd9f80-a352-11e9-8fdf-aeacd0a79ae7.png">
<meta property="og:updated_time" content="2019-07-10T14:45:24.459Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="特征选择">
<meta name="twitter:description" content="whatfeature-selector是一个基础的特征选择工具，主要对以下类型的特征进行选择：  高missing-values 强相关性的特征 对模型预测结果无贡献的或者很小贡献的 特征值只有一个单独变量  通常拿到一个数据集的时候，先用feature-selector筛一遍，主要提供以下五个函数： 12345identify_missing(*)identify_collinear(*)id">
<meta name="twitter:image" content="https://user-images.githubusercontent.com/9191594/60969408-07803800-a352-11e9-9294-cdc1c17b0ae2.png">
  
    <link rel="alternate" href="/atom.xml" title="Moby Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Moby Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">T_blog</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://mobyIsMe.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-特征选择" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/10/特征选择/" class="article-date">
  <time datetime="2019-05-10T12:20:00.000Z" itemprop="datePublished">2019-05-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      特征选择
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="what"><a href="#what" class="headerlink" title="what"></a>what</h2><p>feature-selector是一个基础的特征选择工具，主要对以下类型的特征进行选择：</p>
<ul>
<li>高missing-values</li>
<li>强相关性的特征</li>
<li>对模型预测结果无贡献的或者很小贡献的</li>
<li>特征值只有一个单独变量</li>
</ul>
<p>通常拿到一个数据集的时候，先用feature-selector筛一遍，主要提供以下五个函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">identify_missing(*)</span><br><span class="line">identify_collinear(*)</span><br><span class="line">identify_zero_importance(*)</span><br><span class="line">identify_low_importance(*)</span><br><span class="line">identify_single_unique(*)</span><br></pre></td></tr></table></figure>
<h2 id="how"><a href="#how" class="headerlink" title="how"></a>how</h2><ol>
<li><p>准备dataset</p>
<p>在这里使用kaggle上Home Credit Default Risk Competition的训练数据集，30+万行(150+MB)，可以从原数据集中采样了1万+行数据作练习。数据集采样代码如下：</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">'./appliation_train.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从原数据中采样5%的数据</span></span><br><span class="line">sample = data.sample(frac=<span class="number">0.05</span>)</span><br><span class="line"><span class="comment"># 重新创建索引</span></span><br><span class="line">sample.reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 将采样数据存到'application_train_sample.csv'文件中</span></span><br><span class="line">sample.to_csv(<span class="string">'./application_train_sample.csv'</span>)</span><br></pre></td></tr></table></figure>
<p>2.feature-selector用法<br>2.1 导入数据并创建feaure-selector实例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas  <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> feature_selector <span class="keyword">import</span> FeatureSelector</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">'./application_train_sample.csv'</span>, index_col=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 数据集中TARGET字段为对应样本的label</span></span><br><span class="line">train_labels = data.TARGET</span><br><span class="line"><span class="comment"># 获取all features</span></span><br><span class="line">train_features = data.drop(columns=<span class="string">'TARGET'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 feature-selector 实例，并传入features 和labels</span></span><br><span class="line">fs = FeatureSelector(data=train_features, labels=train_labels)</span><br></pre></td></tr></table></figure>
<p>2.2 特征选取方法</p>
<p>（1）identify_missing</p>
<p>该方法用于选择missing value 百分比大于指定值(通过missing_threshold指定百分比)的feature。该方法能应用于监督学习和非监督学习的特征选择。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选择出missing value 百分比大于60%的特征</span></span><br><span class="line">fs.identify_missing(missing_threshold=<span class="number">0.6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看选择出的特征</span></span><br><span class="line">fs.ops[<span class="string">'missing'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制所有特征missing value百分比的直方图</span></span><br><span class="line">fs.plot_missing()</span><br></pre></td></tr></table></figure>
<p>该方法内部使用pandas 统计数据集中所有feature的missing value 的百分比，然后选择出百分比大于阈值的特征</p>
<p>(2) identify_collinear<br>该方法用于选择相关性大于指定值(通过correlation_threshold指定值)的feature。该方法同样适用于监督学习和非监督学习。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不对feature进行one-hot encoding（默认为False）, 然后选择出相关性大于98%的feature, </span></span><br><span class="line">fs.identify_collinear(correlation_threshold=<span class="number">0.98</span>, one_hot=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看选择的feature</span></span><br><span class="line">fs.ops[<span class="string">'collinear'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制选择的特征的相关性heatmap</span></span><br><span class="line">fs.plot_collinear()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制所有特征的相关性heatmap</span></span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/9191594/60969408-07803800-a352-11e9-9294-cdc1c17b0ae2.png" alt="image"></p>
<p>图2. 选择的特征的相关矩阵图<br><img src="https://user-images.githubusercontent.com/9191594/60969486-35657c80-a352-11e9-966d-f920717d184c.png" alt="image"><br>图3. 所有特征相关矩阵图<br>该方法内部主要执行步骤如下：</p>
<p>(3) identify_zero_importance</p>
<p>该方法用于选择对模型预测结果毫无贡献的feature(即zero importance，从数据集中去除或者保留该feature对模型的结果不会有任何影响)。<br>该方法以及之后identify_low_importance都只适用于监督学习(即需要label,这也是为什么实例化feature-selector时需要传入labels参数的原因）。feature-selector通过用数据集训练一个梯度提升机(Gradient Boosting machine, GBM)，然后由GBM得到每一个feature的重要性分数，对所有特征的重要性分数进行归一化处理，选择出重要性分数等于零的feature。</p>
<p>为了使计算得到的feature重要性分数具有很小的方差，identify_zero_importance内部会对GBM训练多次，取多次训练的平均值，得到最终的feature重要性分数。同时为了防止过拟合，identify_zero_importance内部从数据集中抽取一部分作为验证集，在训练GBM的时候，计算GBM在验证集上的某一metric，当metric满足一定条件时，停止GBM的训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选择zero importance的feature,</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># 参数说明：</span></span><br><span class="line"><span class="comment">#          task: 'classification' / 'regression', 如果数据的模型是分类模型选择'classificaiton',</span></span><br><span class="line"><span class="comment">#                否则选择'regression'</span></span><br><span class="line"><span class="comment">#          eval_metric: 判断提前停止的metric. for example, 'auc' for classification, and 'l2' for regression problem</span></span><br><span class="line"><span class="comment">#          n_iteration: 训练的次数</span></span><br><span class="line"><span class="comment">#          early_stopping: True/False, 是否需要提前停止</span></span><br><span class="line">fs.identify_zero_importance(task=<span class="string">'classification'</span>,</span><br><span class="line">                                             eval_metric=<span class="string">'auc'</span>,</span><br><span class="line">                                             n_iteration=<span class="number">10</span>,</span><br><span class="line">                                             early_stopping=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看选择出的zero importance feature</span></span><br><span class="line">fs.ops[<span class="string">'zero_importance'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制feature importance 关系图</span></span><br><span class="line"><span class="comment"># 参数说明：</span></span><br><span class="line"><span class="comment">#          plot_n: 指定绘制前plot_n个最重要的feature的归一化importance条形图，如图4所示</span></span><br><span class="line"><span class="comment">#          threshold: 指定importance分数累积和的阈值，用于指定图4中的蓝色虚线.</span></span><br><span class="line"><span class="comment">#              蓝色虚线指定了importance累积和达到threshold时，所需要的feature个数。</span></span><br><span class="line"><span class="comment">#              注意：在计算importance累积和之前，对feature列表安装feature importance的大小</span></span><br><span class="line"><span class="comment">#                   进行了降序排序</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#      </span></span><br><span class="line">fs.plot_feature_importances(threshold=<span class="number">0.99</span>, plot_n=<span class="number">12</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/9191594/60969615-8bd2bb00-a352-11e9-916f-d0d33bd292f4.png" alt="image"></p>
<p>图4. 前12个最重要的feature归一化后的importance分数的条形图<br><img src="https://user-images.githubusercontent.com/9191594/60969644-a016b800-a352-11e9-92b7-3927dce6b1ca.png" alt="image"></p>
<p>图5. feature 个数与feature importance累积和的关系图</p>
<p>需要注意GBM训练过程是随机的，所以每次运行identify_zero_importance得到feature importance分数都会发生变化，但按照importance排序之后，至少前几个最重要的feature顺序不会变化。</p>
<p>(4) identify_low_importance</p>
<p>该方法是使用identify_zero_importance计算的结果，选择出对importance累积和达到指定阈值没有贡献的feature（这样说有点拗口），即图5中蓝色虚线之后的feature。该方法只适用于监督学习。identify_low_importance有点类似于PCA中留下主要分量去除不重要的分量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选择出对importance累积和达到99%没有贡献的feature</span></span><br><span class="line">fs.identify_low_importance(cumulative_importance=<span class="number">0.99</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看选择出的feature</span></span><br><span class="line">fs.ops[<span class="string">'low_importance'</span>]</span><br></pre></td></tr></table></figure>
<p>该方法选择出的feature其实包含了zero importance的feature。</p>
<p>(5) identify_single_unique</p>
<p>该方法用于选择只有单个取值的feature，单个值的feature的方差为0，对于模型的训练不会有任何作用（从信息熵的角度看，该feature的熵为0）。该方法可应用于监督学习和非监督学习。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选择出只有单个值的feature</span></span><br><span class="line">fs.identify_single_unique()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看选择出的feature</span></span><br><span class="line">fs.ops[<span class="string">'single_unique'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制所有feature unique value的直方图</span></span><br><span class="line">fs.plot_unique()</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/9191594/60969744-dfdd9f80-a352-11e9-8fdf-aeacd0a79ae7.png" alt="image"></p>
<p>图6. 所有feature unique value的直方图</p>
<p>该方法内部的内部实现很简单，只是通过DataFrame.nunique方法统计了每个feature取值的个数，然后选择出nunique==1等于1的feature。</p>
<p>3.3 从数据集去除选择的特征</p>
<p>3.2中介绍了feature-selector提供的特征选择方法，这些方法从数据集中识别了feature，但并没有从数据集中将这些feature去除。feature-selector中提供了remove方法将选择的特征从数据集中去除，并返回去除特征之后的数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 去除所有类型的特征</span></span><br><span class="line"><span class="comment">#    参数说明：</span></span><br><span class="line"><span class="comment">#       methods: </span></span><br><span class="line"><span class="comment">#               desc:  需要去除哪些类型的特征</span></span><br><span class="line"><span class="comment">#               type:  string / list-like object</span></span><br><span class="line"><span class="comment">#             values:  'all' 或者是 ['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance']</span></span><br><span class="line"><span class="comment">#                      中多个方法名的组合</span></span><br><span class="line"><span class="comment">#      keep_one_hot: </span></span><br><span class="line"><span class="comment">#              desc: 是否需要保留one-hot encoding的特征</span></span><br><span class="line"><span class="comment">#              type: boolean</span></span><br><span class="line"><span class="comment">#              values: True/False</span></span><br><span class="line"><span class="comment">#              default: True</span></span><br><span class="line">train_removed = fs.remove(methods = <span class="string">'all'</span>, keep_one_hot=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>注意：调用remove函数的时候，必须先调用特征选择函数，即identify_*函数。</p>
<p>3.4 一次性选择所有类型的特征</p>
<p>feature-selector除了能每次运行一个identify_*函数来选择一种类型特征外，还可以使用identify_all函数一次性选择5种类型的特征选。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意：</span></span><br><span class="line"><span class="comment"># 少了下面任何一个参数都会报错，raise ValueError</span></span><br><span class="line">fs.identify_all(selection_params = &#123;<span class="string">'missing_threshold'</span>: <span class="number">0.6</span>,    </span><br><span class="line">                                    <span class="string">'correlation_threshold'</span>: <span class="number">0.98</span>, </span><br><span class="line">                                    <span class="string">'task'</span>: <span class="string">'classification'</span>,    </span><br><span class="line">                                    <span class="string">'eval_metric'</span>: <span class="string">'auc'</span>, </span><br><span class="line">                                    <span class="string">'cumulative_importance'</span>: <span class="number">0.99</span>&#125;)</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>结尾<br>feature-selector属于非常基础的特征选择工具，它提供了五种特征的选择函数，每个函数负责选择一种类型的特征。一般情况下，在对某一数据集构建模型之前，都需要考虑从数据集中去除这五种类型的特征，所以feature-selector帮你省去data-science生活中一部分重复性的代码工作。</li>
</ol>
<h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><p><a href="https://zhuanlan.zhihu.com/p/49479702" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/49479702</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mobyIsMe.github.io/2019/05/10/特征选择/" data-id="cjxxhwpr80007m1r0ivvi4k0l" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/数据分析，数据挖掘/">数据分析，数据挖掘</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/06/11/数据预处理常见姿势/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          数据预处理常见姿势
        
      </div>
    </a>
  
  
    <a href="/2018/10/16/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据分析，数据挖掘/">数据分析，数据挖掘</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/数据分析，数据挖掘/" style="font-size: 10px;">数据分析，数据挖掘</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/07/10/一键生成数据报告/">一键生成数据报告</a>
          </li>
        
          <li>
            <a href="/2019/06/11/数据预处理常见姿势/">数据预处理常见姿势</a>
          </li>
        
          <li>
            <a href="/2019/05/10/特征选择/">特征选择</a>
          </li>
        
          <li>
            <a href="/2018/10/16/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2018/10/10/推荐架构/">推荐架构</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 moby<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>