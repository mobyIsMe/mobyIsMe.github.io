<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Moby Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Moby Blog">
<meta property="og:url" content="http://mobyIsMe.github.io/index.html">
<meta property="og:site_name" content="Moby Blog">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Moby Blog">
  
    <link rel="alternate" href="/atom.xml" title="Moby Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Moby Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">T_blog</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://mobyIsMe.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-一键生成数据报告" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/07/10/一键生成数据报告/" class="article-date">
  <time datetime="2019-07-10T14:50:10.000Z" itemprop="datePublished">2019-07-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/07/10/一键生成数据报告/">一键生成数据报告</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>pandas_profiling可以迅速帮我们进行探索数据分析（exploratory data analysis）</p>
</blockquote>
<p>探索性数据分析（EDA）在理解数据集方面起着非常重要的作用。 无论是要构建机器学习模型还是仅仅是从给定数据中获取洞察力的练习，EDA都是首要任务。 虽然不可否认的是EDA非常重要，但执行探索性数据分析的任务与数据集的列数并行增长。</p>
<p>例如：假设有一个包含10行x 2列的数据集。 只需单独指定这两个列名称并绘制执行EDA所需的所有绘图即可。 或者，如果数据集有20列，则需要重复相同的上述练习10次。 现在，还有另一层复杂性，因为“连续变量”和“分类变量”选择的可视化是不同的，因此当数据类型更改时，绘图的类型会发生变化。</p>
<p>鉴于所有这些条件，EDA有时会成为一项繁琐的工作 - 但要记住它全部由一组规则驱动 - 如连续变量的情节<code>boxplot</code>和<code>histogram</code>，测量<code>缺失值</code>，如果它是分类的，则计算<code>频率</code>变量 - 从而为我们提供自动化事物的机会。 这是这个python模块<code>pandas_profiling</code>的基础，可以帮助自动化第一级EDA。</p>
<p>对于每列，以下统计信息（如果与列类型相关）将显示在交互式HTML报告中：</p>
<ul>
<li><p><strong>要点</strong>：类型，唯一值，缺失值</p>
</li>
<li><p><strong>分位数统计</strong> :如最小值，Q1，中位数，Q3，最大值，范围，四分位数间距</p>
</li>
<li><p><strong>描述性统计</strong> :如均值，模式，标准差，总和，中位数绝对偏差，变异系数，峰度，偏度</p>
</li>
<li><p><strong>最常见的值</strong></p>
</li>
<li><p><strong>直方图</strong></p>
</li>
<li><p><strong>相关性</strong> 突出显示高度相关的变量，Spearman和Pearson矩阵</p>
</li>
</ul>
<p>以Titanic数据集为例：</p>
<ol>
<li>首先，让我们导入数据并使用pandas来检索一些描述性统计信息：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># linear algebra</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># data processing, CSV file I/O (e.g. pd.read_csv)</span></span><br><span class="line"><span class="keyword">import</span> pandas_profiling <span class="keyword">as</span> pp</span><br><span class="line">train = pd.read_csv(<span class="string">"../input/train.csv"</span>, encoding=<span class="string">'UTF-8'</span>, parse_dates = [<span class="string">'project_submitted_datetime'</span>])</span><br><span class="line"></span><br><span class="line">pp.ProfileReport(train) <span class="comment">#查看数据整体情况</span></span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/9191594/60982712-3fe04000-a36b-11e9-8a60-f9aaab02be3a.png" alt="image"><br><img src="https://user-images.githubusercontent.com/9191594/60982764-55556a00-a36b-11e9-9fe1-c6c201e46ed4.png" alt="image"><br><img src="https://user-images.githubusercontent.com/9191594/60982816-6f8f4800-a36b-11e9-8f4d-5154e7eb2f73.png" alt="image"><br><img src="https://user-images.githubusercontent.com/9191594/60982879-8afa5300-a36b-11e9-8f55-8ae51021a9e6.png" alt="image"><br><img src="https://user-images.githubusercontent.com/9191594/60982914-9baac900-a36b-11e9-9a9d-14ed1941b246.png" alt="image"><br><img src="https://user-images.githubusercontent.com/9191594/60983136-078d3180-a36c-11e9-9596-58e1ba5c121f.png" alt="image"><br><img src="https://user-images.githubusercontent.com/9191594/60983297-47541900-a36c-11e9-92ea-c27249c85cce.png" alt="image"><br>右下角有一个toggle details，这里详细描述了每一个属性的统计结果：<br>以sibsip为例：<br><img src="https://user-images.githubusercontent.com/9191594/60983488-969a4980-a36c-11e9-9244-6b249297ed02.png" alt="image"><br>panda分别从统计维度，直方图，众数，极值4个维度来描述这个亲戚属性<br>其中统计维度又可分为数值统计（中位数，4分位数，5%，95%的数值）和描述性统计（标准差，峰度，均值，方差等）<br><img src="https://user-images.githubusercontent.com/9191594/60983547-af0a6400-a36c-11e9-8c3d-35e1d536a774.png" alt="image"><br><img src="https://user-images.githubusercontent.com/9191594/60983899-6010fe80-a36d-11e9-8596-9bba6c288680.png" alt="image"><br><img src="https://user-images.githubusercontent.com/9191594/60983930-6bfcc080-a36d-11e9-8963-67b22d087420.png" alt="image"><br><img src="https://user-images.githubusercontent.com/9191594/60983952-73bc6500-a36d-11e9-809e-f4373c6aba75.png" alt="image"><br>相关系数：<br><img src="https://user-images.githubusercontent.com/9191594/60986678-9604b180-a372-11e9-8008-e8b3d213bfe4.png" alt="image"><br>对比一下describe生成的报告：<br><img src="https://user-images.githubusercontent.com/9191594/60984706-c0547000-a36e-11e9-94f3-6b3846028659.png" alt="image"></p>
<p>最厉害的是，这还支持生成交互式的HTML的数据报告：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pfr = pandas_profiling.ProfileReport(titanic)</span><br><span class="line">pfr.to_file(<span class="string">'report.html'</span>)</span><br></pre></td></tr></table></figure>
<p>生成的报告在这里：<br>链接: <a href="https://pan.baidu.com/s/1fmiry895dirBGBRtatY5hA" target="_blank" rel="noopener">https://pan.baidu.com/s/1fmiry895dirBGBRtatY5hA</a> 提取码: aywh </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mobyIsMe.github.io/2019/07/10/一键生成数据报告/" data-id="cjxxhv3t80002k2r0h75xmnxw" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-数据预处理常见姿势" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/06/11/数据预处理常见姿势/" class="article-date">
  <time datetime="2019-06-10T16:34:37.000Z" itemprop="datePublished">2019-06-11</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/06/11/数据预处理常见姿势/">数据预处理常见姿势</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="数据清洗的6个小pose："><a href="#数据清洗的6个小pose：" class="headerlink" title="数据清洗的6个小pose："></a>数据清洗的6个小pose：</h3><ul>
<li><p>删除 DataFrame 中的不必要 columns，rows</p>
<p><code>df.drop(to_drop_columns, inplace=True, axis=1)</code></p>
</li>
<li><p>改变 DataFrame 的 index</p>
<p><code>df.set_index(column_name, inplace=True)</code></p>
</li>
<li><p>使用 .str() 方法来清洗 columns<br>筛选出要清洗的列，改变数据格式<br><code>np.where(condition, then, else)</code><br>np.where函数在一个嵌套的结构中被调用，condition是一个通过st.contains()得到的布尔的Series。contains()方法与Python内建的in关键字一样，用于发现一个个体是否发生在一个迭代器中。</p>
</li>
</ul>
<ul>
<li><p>使用 DataFrame.applymap() 函数按元素清洗整个数据集<br>applymap()方法从DataFrame中提取每个元素，传递到函数中，     然后覆盖原来的值</p>
</li>
<li><p>重命名 columns 为一组更易识别的标签</p>
<p><code>df.rename(columns=new_names, inplace=True)</code></p>
</li>
</ul>
<h3 id="缺失值处理："><a href="#缺失值处理：" class="headerlink" title="缺失值处理："></a>缺失值处理：</h3><ul>
<li>删除记录：<ul>
<li><strong>优点：</strong>最简单粗暴；</li>
<li><strong>缺点：</strong></li>
<li>牺牲了大量的数据，通过减少历史数据换取完整的信息，这样可能丢失了很多隐藏的重要信息；</li>
<li>当缺失数据比例较大时，特别是缺失数据非随机分布时，直接删除可能会导致数据发生偏离，比如原本的正态分布变为非正太；</li>
<li>这种方法在样本数据量十分大且缺失值不多的情况下非常有效，但如果样本量本身不大且缺失也不少，那么不建议使用。</li>
<li>可以使用 pandas 的 dropna 来直接删除有缺失值的特征。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#删除数据表中含有空值的行</span></span><br><span class="line">df.dropna(how=<span class="string">'any'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>数据填补：对缺失值的插补大体可分为两种：替换缺失值，拟合缺失值，虚拟变量。替换是通过数据中非缺失数据的相似性来填补，其核心思想是发现相同群体的共同特征，拟合是通过其他特征建模来填补，虚拟变量是衍生的新变量代替缺失值。<ul>
<li>模型预测  ：<ul>
<li>k-means:</li>
<li>回归预测</li>
<li>最大似然估计：EM.该方法比删除个案和单值插补更有吸引力，它一个重要前提：适用于大样本。有效样本的数量足够以保证ML估计值是渐近无偏的并服从正态分布。但是这种方法可能会陷入局部极值，收敛速度也不是很快，并且计算很复杂，且仅限于线性模型。   </li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>* 均值插补：
    * 对于定类数据：使用 **众数**（mode）填补，比如一个学校的男生和女生的数量，男生500人，女生50人，那么对于其余的缺失值我们会用人数较多的男生来填补。
    * 对于定量（定比）数据：使用**平均数**（mean）或**中位数**（median）填补，比如一个班级学生的身高特征，对于一些同学缺失的身高值就可以使用全班同学身高的平均值或中位数来填补。一般如果特征分布为正太分布时，使用平均值效果比较好，而当分布由于异常值存在而不是正太分布的情况下，使用中位数效果比较好。

    &gt; 注：此方法虽然简单，但是不够精准，可能会引入噪声，或者会改变特征原有的分布。
</code></pre><p>下图左为填补前的特征分布，图右为填补后的分布，明显发生了畸变。因此，如果缺失值是随机性的，那么用平均值比较适合保证无偏，否则会改变原分布。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用price均值对NA进行填充</span></span><br><span class="line">df[<span class="string">'price'</span>].fillna(df[<span class="string">'price'</span>].mean())</span><br><span class="line">df[<span class="string">'price'</span>].fillna(df[<span class="string">'price'</span>].median())</span><br></pre></td></tr></table></figure>
<ul>
<li>不处理。补齐处理只是将未知值补以我们的主观估计值，不一定完全符合客观事实，在对不完备信息进行补齐处理的同时，我们或多或少地改变了原始的信息系统。而且，对空值不正确的填充往往将新的噪声引入数据中，使挖掘任务产生错误的结果。因此，在许多情况下，我们还是希望在保持原始信息不发生变化的前提下对信息系统进行处理。在实际应用中，一些模型无法应对具有缺失值的数据，因此要对缺失值进行处理。然而还有一些模型本身就可以应对具有缺失值的数据，此时无需对数据进行处理，比如Xgboost，rfr等高级模型。</li>
</ul>
<p>思维导图总结为：<br><img src="https://user-images.githubusercontent.com/9191594/60988615-8a1aee80-a376-11e9-82b8-f426b49aaae5.png" alt="image"></p>
<h3 id="离群点检测："><a href="#离群点检测：" class="headerlink" title="离群点检测："></a>离群点检测：</h3><p>离群点的概率定义：离群点是一个对象，关于数据的概率分布模型，它具有低概率。这种情况的前提是必须知道数据集服从什么分布，如果估计错误就造成了重尾分布。</p>
<ul>
<li><p>基于统计的方法：我们可以直接使用describe()来观察数据的统计性描述，或者简单使用散点图也能很清晰的观察到异常值的存在。</p>
<ul>
<li>3∂原则：这个原则有个条件：数据需要服从正态分布。在3∂原则下，异常值如超过3倍标准差，那么可以将其视为异常值。正负3∂的概率是99.7%，那么距离平均值3∂之外的值出现的概率为P(|x-u| &gt; 3∂) &lt;= 0.003，属于极个别的小概率事件。如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。<br>*<img src="https://user-images.githubusercontent.com/9191594/60988760-dcf4a600-a376-11e9-97b0-886154edfbcf.png" alt="image"></li>
<li>箱型图：利用箱型图的四分位距（IQR）对异常值进行检测，四分位距(IQR)就是上四分位与下四分位的差值。而我们通过IQR的1.5倍为标准，规定：超过（上四分位+1.5倍IQR距离，或者下四分位-1.5倍IQR距离）的点为异常值。</li>
<li><p><img src="https://user-images.githubusercontent.com/9191594/60988799-f7c71a80-a376-11e9-9179-90de33427cda.png" alt="image"></p>
</li>
<li><p>概率分布模型检测：计算对象符合该模型的概率，把具有低概率的对象视为异常点。如果模型是簇的集合，则异常是不显著属于任何簇的对象；如果模型是回归时，异常是相对远离预测值的对象</p>
</li>
</ul>
</li>
<li><p>基于KNN：需要注意的是：离群点得分对k的取值高度敏感。如果k太小，则少量的邻近离群点可能导致较低的离群点得分；如果K太大，则点数少于k的簇中所有的对象可能都成了离群点。为了使该方案对于k的选取更具有鲁棒性，可以使用k个最近邻的平均距离。</p>
<ul>
<li>优缺点：（1）简单；（2）缺点：基于邻近度的方法需要O(m2)时间，大数据集不适用；（3）该方法对参数的选择也是敏感的；（4）不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。</li>
</ul>
</li>
<li>基于密度：一种常用的定义密度的方法是，定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。另一种密度定义是使用DBSCAN聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离d内对象的个数。<ul>
<li>优缺点：（1）给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理；（2）与基于距离的方法一样，这些方法必然具有O(m2)的时间复杂度。对于低维数据使用特定的数据结构可以达到O(mlogm)；（3）参数选择是困难的。虽然LOF算法通过观察不同的k值，然后取得最大离群点得分来处理该问题，但是，仍然需要选择这些值的上下界。</li>
</ul>
</li>
</ul>
<ul>
<li>基于聚类的方法<br>一个对象是基于聚类的离群点，如果该对象不强属于任何簇，那么该对象属于离群点。离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：结构是否有效。这也是k-means算法的缺点，对离群点敏感。为了处理该问题，可以使用如下方法：对象聚类，删除离群点，对象再次聚类（这个不能保证产生最优结果）。<br>优缺点：（1）基于线性和接近线性复杂度（k均值）的聚类技术来发现离群点可能是高度有效的；（2）簇的定义通常是离群点的补，因此可能同时发现簇和离群点；（3）产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性；（4）聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。</li>
</ul>
<h3 id="异常值的处理方法"><a href="#异常值的处理方法" class="headerlink" title="异常值的处理方法"></a>异常值的处理方法</h3><ul>
<li>删除含有异常值的记录：直接将含有异常值的记录删除；</li>
<li>视为缺失值：将异常值视为缺失值，利用缺失值处理的方法进行处理；</li>
<li>平均值修正：可用前后两个观测值的平均值修正该异常值；</li>
<li>不处理：直接在具有异常值的数据集上进行数据挖掘；</li>
</ul>
<p>是否要删除异常值可根据实际情况考虑。因为一些模型对异常值不很敏感，即使有异常值也不影响模型效果，但是一些模型比如逻辑回归LR对异常值很敏感，如果不进行处理，可能会出现过拟合等非常差的效果。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mobyIsMe.github.io/2019/06/11/数据预处理常见姿势/" data-id="cjxxhv3tv0006k2r0djpw1p2x" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-特征选择" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/10/特征选择/" class="article-date">
  <time datetime="2019-05-10T12:20:00.000Z" itemprop="datePublished">2019-05-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/05/10/特征选择/">特征选择</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="what"><a href="#what" class="headerlink" title="what"></a>what</h2><p>feature-selector是一个基础的特征选择工具，主要对以下类型的特征进行选择：</p>
<ul>
<li>高missing-values</li>
<li>强相关性的特征</li>
<li>对模型预测结果无贡献的或者很小贡献的</li>
<li>特征值只有一个单独变量</li>
</ul>
<p>通常拿到一个数据集的时候，先用feature-selector筛一遍，主要提供以下五个函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">identify_missing(*)</span><br><span class="line">identify_collinear(*)</span><br><span class="line">identify_zero_importance(*)</span><br><span class="line">identify_low_importance(*)</span><br><span class="line">identify_single_unique(*)</span><br></pre></td></tr></table></figure>
<h2 id="how"><a href="#how" class="headerlink" title="how"></a>how</h2><ol>
<li><p>准备dataset</p>
<p>在这里使用kaggle上Home Credit Default Risk Competition的训练数据集，30+万行(150+MB)，可以从原数据集中采样了1万+行数据作练习。数据集采样代码如下：</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">'./appliation_train.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从原数据中采样5%的数据</span></span><br><span class="line">sample = data.sample(frac=<span class="number">0.05</span>)</span><br><span class="line"><span class="comment"># 重新创建索引</span></span><br><span class="line">sample.reset_index(drop=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 将采样数据存到'application_train_sample.csv'文件中</span></span><br><span class="line">sample.to_csv(<span class="string">'./application_train_sample.csv'</span>)</span><br></pre></td></tr></table></figure>
<p>2.feature-selector用法<br>2.1 导入数据并创建feaure-selector实例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas  <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> feature_selector <span class="keyword">import</span> FeatureSelector</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">'./application_train_sample.csv'</span>, index_col=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 数据集中TARGET字段为对应样本的label</span></span><br><span class="line">train_labels = data.TARGET</span><br><span class="line"><span class="comment"># 获取all features</span></span><br><span class="line">train_features = data.drop(columns=<span class="string">'TARGET'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 feature-selector 实例，并传入features 和labels</span></span><br><span class="line">fs = FeatureSelector(data=train_features, labels=train_labels)</span><br></pre></td></tr></table></figure>
<p>2.2 特征选取方法</p>
<p>（1）identify_missing</p>
<p>该方法用于选择missing value 百分比大于指定值(通过missing_threshold指定百分比)的feature。该方法能应用于监督学习和非监督学习的特征选择。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选择出missing value 百分比大于60%的特征</span></span><br><span class="line">fs.identify_missing(missing_threshold=<span class="number">0.6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看选择出的特征</span></span><br><span class="line">fs.ops[<span class="string">'missing'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制所有特征missing value百分比的直方图</span></span><br><span class="line">fs.plot_missing()</span><br></pre></td></tr></table></figure>
<p>该方法内部使用pandas 统计数据集中所有feature的missing value 的百分比，然后选择出百分比大于阈值的特征</p>
<p>(2) identify_collinear<br>该方法用于选择相关性大于指定值(通过correlation_threshold指定值)的feature。该方法同样适用于监督学习和非监督学习。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不对feature进行one-hot encoding（默认为False）, 然后选择出相关性大于98%的feature, </span></span><br><span class="line">fs.identify_collinear(correlation_threshold=<span class="number">0.98</span>, one_hot=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看选择的feature</span></span><br><span class="line">fs.ops[<span class="string">'collinear'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制选择的特征的相关性heatmap</span></span><br><span class="line">fs.plot_collinear()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制所有特征的相关性heatmap</span></span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/9191594/60969408-07803800-a352-11e9-9294-cdc1c17b0ae2.png" alt="image"></p>
<p>图2. 选择的特征的相关矩阵图<br><img src="https://user-images.githubusercontent.com/9191594/60969486-35657c80-a352-11e9-966d-f920717d184c.png" alt="image"><br>图3. 所有特征相关矩阵图<br>该方法内部主要执行步骤如下：</p>
<p>(3) identify_zero_importance</p>
<p>该方法用于选择对模型预测结果毫无贡献的feature(即zero importance，从数据集中去除或者保留该feature对模型的结果不会有任何影响)。<br>该方法以及之后identify_low_importance都只适用于监督学习(即需要label,这也是为什么实例化feature-selector时需要传入labels参数的原因）。feature-selector通过用数据集训练一个梯度提升机(Gradient Boosting machine, GBM)，然后由GBM得到每一个feature的重要性分数，对所有特征的重要性分数进行归一化处理，选择出重要性分数等于零的feature。</p>
<p>为了使计算得到的feature重要性分数具有很小的方差，identify_zero_importance内部会对GBM训练多次，取多次训练的平均值，得到最终的feature重要性分数。同时为了防止过拟合，identify_zero_importance内部从数据集中抽取一部分作为验证集，在训练GBM的时候，计算GBM在验证集上的某一metric，当metric满足一定条件时，停止GBM的训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选择zero importance的feature,</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># 参数说明：</span></span><br><span class="line"><span class="comment">#          task: 'classification' / 'regression', 如果数据的模型是分类模型选择'classificaiton',</span></span><br><span class="line"><span class="comment">#                否则选择'regression'</span></span><br><span class="line"><span class="comment">#          eval_metric: 判断提前停止的metric. for example, 'auc' for classification, and 'l2' for regression problem</span></span><br><span class="line"><span class="comment">#          n_iteration: 训练的次数</span></span><br><span class="line"><span class="comment">#          early_stopping: True/False, 是否需要提前停止</span></span><br><span class="line">fs.identify_zero_importance(task=<span class="string">'classification'</span>,</span><br><span class="line">                                             eval_metric=<span class="string">'auc'</span>,</span><br><span class="line">                                             n_iteration=<span class="number">10</span>,</span><br><span class="line">                                             early_stopping=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看选择出的zero importance feature</span></span><br><span class="line">fs.ops[<span class="string">'zero_importance'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制feature importance 关系图</span></span><br><span class="line"><span class="comment"># 参数说明：</span></span><br><span class="line"><span class="comment">#          plot_n: 指定绘制前plot_n个最重要的feature的归一化importance条形图，如图4所示</span></span><br><span class="line"><span class="comment">#          threshold: 指定importance分数累积和的阈值，用于指定图4中的蓝色虚线.</span></span><br><span class="line"><span class="comment">#              蓝色虚线指定了importance累积和达到threshold时，所需要的feature个数。</span></span><br><span class="line"><span class="comment">#              注意：在计算importance累积和之前，对feature列表安装feature importance的大小</span></span><br><span class="line"><span class="comment">#                   进行了降序排序</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#      </span></span><br><span class="line">fs.plot_feature_importances(threshold=<span class="number">0.99</span>, plot_n=<span class="number">12</span>)</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/9191594/60969615-8bd2bb00-a352-11e9-916f-d0d33bd292f4.png" alt="image"></p>
<p>图4. 前12个最重要的feature归一化后的importance分数的条形图<br><img src="https://user-images.githubusercontent.com/9191594/60969644-a016b800-a352-11e9-92b7-3927dce6b1ca.png" alt="image"></p>
<p>图5. feature 个数与feature importance累积和的关系图</p>
<p>需要注意GBM训练过程是随机的，所以每次运行identify_zero_importance得到feature importance分数都会发生变化，但按照importance排序之后，至少前几个最重要的feature顺序不会变化。</p>
<p>(4) identify_low_importance</p>
<p>该方法是使用identify_zero_importance计算的结果，选择出对importance累积和达到指定阈值没有贡献的feature（这样说有点拗口），即图5中蓝色虚线之后的feature。该方法只适用于监督学习。identify_low_importance有点类似于PCA中留下主要分量去除不重要的分量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选择出对importance累积和达到99%没有贡献的feature</span></span><br><span class="line">fs.identify_low_importance(cumulative_importance=<span class="number">0.99</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看选择出的feature</span></span><br><span class="line">fs.ops[<span class="string">'low_importance'</span>]</span><br></pre></td></tr></table></figure>
<p>该方法选择出的feature其实包含了zero importance的feature。</p>
<p>(5) identify_single_unique</p>
<p>该方法用于选择只有单个取值的feature，单个值的feature的方差为0，对于模型的训练不会有任何作用（从信息熵的角度看，该feature的熵为0）。该方法可应用于监督学习和非监督学习。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选择出只有单个值的feature</span></span><br><span class="line">fs.identify_single_unique()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看选择出的feature</span></span><br><span class="line">fs.ops[<span class="string">'single_unique'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制所有feature unique value的直方图</span></span><br><span class="line">fs.plot_unique()</span><br></pre></td></tr></table></figure>
<p><img src="https://user-images.githubusercontent.com/9191594/60969744-dfdd9f80-a352-11e9-8fdf-aeacd0a79ae7.png" alt="image"></p>
<p>图6. 所有feature unique value的直方图</p>
<p>该方法内部的内部实现很简单，只是通过DataFrame.nunique方法统计了每个feature取值的个数，然后选择出nunique==1等于1的feature。</p>
<p>3.3 从数据集去除选择的特征</p>
<p>3.2中介绍了feature-selector提供的特征选择方法，这些方法从数据集中识别了feature，但并没有从数据集中将这些feature去除。feature-selector中提供了remove方法将选择的特征从数据集中去除，并返回去除特征之后的数据集。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 去除所有类型的特征</span></span><br><span class="line"><span class="comment">#    参数说明：</span></span><br><span class="line"><span class="comment">#       methods: </span></span><br><span class="line"><span class="comment">#               desc:  需要去除哪些类型的特征</span></span><br><span class="line"><span class="comment">#               type:  string / list-like object</span></span><br><span class="line"><span class="comment">#             values:  'all' 或者是 ['missing', 'single_unique', 'collinear', 'zero_importance', 'low_importance']</span></span><br><span class="line"><span class="comment">#                      中多个方法名的组合</span></span><br><span class="line"><span class="comment">#      keep_one_hot: </span></span><br><span class="line"><span class="comment">#              desc: 是否需要保留one-hot encoding的特征</span></span><br><span class="line"><span class="comment">#              type: boolean</span></span><br><span class="line"><span class="comment">#              values: True/False</span></span><br><span class="line"><span class="comment">#              default: True</span></span><br><span class="line">train_removed = fs.remove(methods = <span class="string">'all'</span>, keep_one_hot=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>注意：调用remove函数的时候，必须先调用特征选择函数，即identify_*函数。</p>
<p>3.4 一次性选择所有类型的特征</p>
<p>feature-selector除了能每次运行一个identify_*函数来选择一种类型特征外，还可以使用identify_all函数一次性选择5种类型的特征选。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意：</span></span><br><span class="line"><span class="comment"># 少了下面任何一个参数都会报错，raise ValueError</span></span><br><span class="line">fs.identify_all(selection_params = &#123;<span class="string">'missing_threshold'</span>: <span class="number">0.6</span>,    </span><br><span class="line">                                    <span class="string">'correlation_threshold'</span>: <span class="number">0.98</span>, </span><br><span class="line">                                    <span class="string">'task'</span>: <span class="string">'classification'</span>,    </span><br><span class="line">                                    <span class="string">'eval_metric'</span>: <span class="string">'auc'</span>, </span><br><span class="line">                                    <span class="string">'cumulative_importance'</span>: <span class="number">0.99</span>&#125;)</span><br></pre></td></tr></table></figure>
<ol start="4">
<li>结尾<br>feature-selector属于非常基础的特征选择工具，它提供了五种特征的选择函数，每个函数负责选择一种类型的特征。一般情况下，在对某一数据集构建模型之前，都需要考虑从数据集中去除这五种类型的特征，所以feature-selector帮你省去data-science生活中一部分重复性的代码工作。</li>
</ol>
<h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><p><a href="https://zhuanlan.zhihu.com/p/49479702" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/49479702</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mobyIsMe.github.io/2019/05/10/特征选择/" data-id="cjxxhv3tw0007k2r07g1le2nk" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/数据分析，数据挖掘/">数据分析，数据挖掘</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/16/hello-world/" class="article-date">
  <time datetime="2018-10-16T08:10:30.000Z" itemprop="datePublished">2018-10-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/16/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mobyIsMe.github.io/2018/10/16/hello-world/" data-id="cjxxhv3t00000k2r04vndhfib" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-推荐架构" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/10/推荐架构/" class="article-date">
  <time datetime="2018-10-10T12:52:47.000Z" itemprop="datePublished">2018-10-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/10/推荐架构/">推荐架构</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>[TOC]</p>
<h4 id="推荐引擎算法"><a href="#推荐引擎算法" class="headerlink" title="推荐引擎算法"></a>推荐引擎算法</h4><h4 id="协同过滤推荐算法"><a href="#协同过滤推荐算法" class="headerlink" title="协同过滤推荐算法"></a>协同过滤推荐算法</h4><p>在一个推荐系统中，存在三类关系：用户与用户（U-U矩阵）、物品与物品（V-V矩阵）和用户与物品（U-V矩阵）。</p>
<h5 id="关系矩阵与矩阵计算"><a href="#关系矩阵与矩阵计算" class="headerlink" title="关系矩阵与矩阵计算"></a>关系矩阵与矩阵计算</h5><h6 id="U-U矩阵"><a href="#U-U矩阵" class="headerlink" title="U-U矩阵"></a>U-U矩阵</h6><p><strong>算法原理</strong><br>在基于用户相似度的协同过滤中，用户相似度的计算是基本前提。Pearson相关系数主要用于度量两个变量 i 和 j 之间的相关性，取值范围是+1（强正相关）到-1（强负相关）<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-29/94900278.jpg" alt=""></p>
<p><strong>I</strong>ij为用户 i 和 j 共同评价过的物品的集合，c 是这个集合中的物品元素，<br><strong>r</strong>ic是用户 i 对物品 c 的评价值，<br><strong>算法流程：</strong></p>
<p>算法输入：用户行为日志。 </p>
<p>算法输出：基于协同的用户相似度矩阵。</p>
<ul>
<li>A. 从用户行为日志中获取用户与物品之间的关系数据，即用户对物品的评分数据。 </li>
<li>B. 对于n个用户，依次计算用户1与其他n-1个用户的相似度；再计算用户2与其他n-2个用户的相似度。对于其中任意两个用户 i 和 j ：<ul>
<li>a) 查找两个用户共同评价过的物品集</li>
<li>b) 分别计算用户 i 和对物品 j 的平均评价</li>
<li>c) 计算用户间相似度，得到用户 i 和 j 的相似度。</li>
</ul>
</li>
<li>C. 将计算得到的相似度结果存储于数据库中。</li>
</ul>
<h6 id="V-V矩阵"><a href="#V-V矩阵" class="headerlink" title="V-V矩阵"></a>V-V矩阵</h6><p><strong>算法原理：</strong><br>在基于物品相似度的协同过滤中，物品相似度的计算是基本前提。将物品的评价数值抽象为n维用户空间中的列向量<strong>x</strong>i,<strong>x</strong>j，分别代表用户 u 对物品<strong>x</strong>i，<strong>x</strong>j的评价值，<br>使用修正的余弦相似度，计算公式为：<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-29/48740525.jpg" alt=""><br><strong>U</strong>xi,xj是对物品<strong>x</strong>i,<strong>x</strong>j共同评价过的用户集合，</p>
<p><strong>算法流程</strong></p>
<p>算法输入：用户行为日志。</p>
<p>算法输出：基于协同的物品相似度矩阵。 </p>
<ul>
<li>A. 从用户行为日志中获取用户与物品之间的关系数据，即用户对物品的评分数据。 </li>
<li>B. 对于n个物品，依次计算物品1与其他n-1个物品的相似度；再计算物品2与其他n-2个物品的相似度。对于其中任意两个物品 i 和 j： <ul>
<li>a) 查找对物品 i 和 j 共同评价过的用户集<strong>U</strong>ij</li>
<li>b) 分别计算用户对物品 i 和 j 的平均评价：<strong>r</strong>i,<strong>r</strong>j</li>
<li>c) 计算物品间相似度，得到物品 i 和 j 的相似度。</li>
</ul>
</li>
<li><p>C. 将计算得到的相似度结果存储于数据库中。</p>
<h6 id="U-V矩阵"><a href="#U-V矩阵" class="headerlink" title="U-V矩阵"></a>U-V矩阵</h6><p>在真实的推荐系统中，一方面U-V矩阵的行列数会随着用户和物品数量变得庞大，另一方面，因为用户实际上只能对有限数量的物品做出评价，所以U-V矩阵的内部会非常稀疏。系统在直接处理这些庞大稀疏矩阵时，耗费的时间、内存和计算资源都十分巨大。因此需要采取降低计算复杂度的方法。矩阵分解技术是一种有效降低矩阵计算复杂的方法，它的实质是将高维矩阵进行有效降维。</p>
</li>
<li><p>奇异值分解（SVD）</p>
<p>  SVD将给定矩阵分解为3个矩阵的乘积：<br>  <img src="http://phal1xgub.bkt.clouddn.com/18-10-29/57088537.jpg" alt=""><br>  中间的变量为对角矩阵，其对角线上的值为矩阵M的奇异值，按大小排列，代表着矩阵M的重要特征。将SVD用在推荐系统上，其意义是将一个系数的评分矩阵M分解为表示用户特性的U矩阵，表示物品特性的V矩阵，以及表示用户和物品相关性的矩阵。<br>  M = 用户特性的U矩阵<em> 用户和物品相关性的矩阵 </em>物品特性的V矩阵</p>
</li>
<li><p>主成分分析（PCA）<br> 在推荐系统中，对于有较多属性的物品（物品的信息用向量<br> <strong>m</strong> [<strong>i</strong>1,<strong>i</strong>2,<strong>i</strong>3….<strong>i</strong>n]<br> 表示）可用PCA处理进行降维，将m×n的物品矩阵转化为m×k的新矩阵。</p>
</li>
</ul>
<h5 id="memory-based的协同过滤算法"><a href="#memory-based的协同过滤算法" class="headerlink" title="memory-based的协同过滤算法"></a>memory-based的协同过滤算法</h5><p>基于记忆的cf可以分为两个步骤，<br>第一：找到目标之间的相似性<br>第二：根据相似性去推荐</p>
<h6 id="user-cf"><a href="#user-cf" class="headerlink" title="user-cf:"></a>user-cf:</h6><p> (1) 找到和目标用户兴趣相似的用户集合。<br> (2) 找到这个集合中的用户喜欢的，且目标用户没有听说过的物品推荐给目标用户。<br><strong>算法流程</strong><br>算法输入：用户行为日志，基于协同的用户相似性矩阵。<br>算法输出：初始推荐结果</p>
<ul>
<li>A. 访问用户行为日志，获取近期变化的用户ID集合U。</li>
<li>B. 针对集合U中每个用户 u：<ul>
<li>a) 访问用户相似矩阵，获取与用户相似的用户合集N(u)。 </li>
<li>b) 对于N(u)中的每一个用户ui： </li>
<li>获取与用户ui有关联的物品合集<strong>M</strong>ui</li>
<li>针对物品合集<strong>M</strong>ui中的每个物品，计算用户偏好值。 </li>
<li>c) 对集M(u)中的所有物品进行按照用户偏好进行加权、去重、排序。 </li>
<li>d) 取Top-N个物品，为每个物品赋予解释。 </li>
<li>e) 保存Top-N个物品到初始推荐列表中。</li>
</ul>
</li>
</ul>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-29/6888260.jpg" alt=""></p>
<p>给定用户u和用户v，令N(u)表示用户u曾经有过正反馈的物品集合，令N(v) 为用户v曾经有过正反馈的物品集合。那么，我们可以通过如下的Jaccard公式简单地计算u和v的 兴趣相似度</p>
<p><strong>适用性</strong></p>
<p>由于需计算用户相似度矩阵，基于用户的协同过滤算法适用于用户较少的场合； 由于时效性较强，该方法适用于用户个性化兴趣不太明显的领域。</p>
<h6 id="item-cf"><a href="#item-cf" class="headerlink" title="item-cf"></a>item-cf</h6><p>基于物品的协同过滤（item-based collaborative filtering）算法是目前业界应用最多的算法。无论是亚马逊网，还是Netflix、Hulu、Youtube，其推荐算法的基础都是该算法。</p>
<p><strong>算法原理</strong></p>
<p>基于物品的协同过滤算法给用户推荐那些和他们之前喜欢的物品相似的物品。</p>
<p>temCF算法并不利用物品的内容属性计算物品之间的相似度，它主要通过分析用户的行为记录计算物品之间的相似度。该算法认为，物品A和物品B具有很大的相似度是因为喜欢物品A的用户大都也喜欢物品B。</p>
<p><strong>算法流程</strong></p>
<p>假设用户为<strong>U</strong>i,物品<strong>M</strong>j,用户对物品的评分是<strong>r</strong>ij，<br>根据用户对物品<strong>M</strong>j的历史行为数据计算物品<strong>M</strong>j与其他已评分物品的相似度 <strong>Sim</strong>(j,i)，构成相似度物品集合<strong>N</strong>(u),根据所有物品<strong>N</strong>(u)的评分情况，选出N(u)中目标用户可能喜欢的且没有观看过的推荐给目标用户并预测评分。<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-29/55036067.jpg" alt=""><br>ItemCF通过下面的公式来计算用户对物品的喜好程度：<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-29/23120071.jpg" alt=""></p>
<p><strong>算法步骤：</strong></p>
<p>算法输入：用户行为日志，基于协同的物品相似性矩阵<br>算法输出：初始推荐结果</p>
<ul>
<li>A. 访问用户行为日志，获取该用户最近浏览过物品的用户集合U。 </li>
<li>B. 针对集合U中每个用户u：<ul>
<li>a) 访问用户相似矩阵，获取与用户相似的用户合集N(u)。 </li>
<li>b) 访问物品相似矩阵，获取与M(u)相似的物品合集N(u)。 </li>
<li>c) 针对物品合集M(ui)中的每个物品，计算用户偏好值。 </li>
<li>d) 根据用户偏好值，对N(u)的物品进行排序。 </li>
<li>e) 取Top-N个物品，为每个物品赋予解释。 </li>
<li>f) 保存Top-N个物品到初始推荐列表中。</li>
</ul>
</li>
</ul>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-29/44895826.jpg" alt=""><br>  这里，分母|N(i)|是喜欢物品i的用户数，而分子 N(i)交N(j) 是同时喜欢物品i和物品j的用户数。因此，上述公式可以理解为喜欢物品i的用户中有多少比例的用户也喜欢物品j。</p>
<p> <strong>适用性</strong><br>适用于物品数明显小于用户数的场合； 长尾物品丰富，用户个性化需求强烈的领域。</p>
<h6 id="UserCF和ItemCF的比较"><a href="#UserCF和ItemCF的比较" class="headerlink" title="UserCF和ItemCF的比较"></a>UserCF和ItemCF的比较</h6><p>  <img src="http://phal1xgub.bkt.clouddn.com/18-10-29/80630137.jpg" alt=""></p>
<h4 id="content-basesd："><a href="#content-basesd：" class="headerlink" title="content-basesd："></a>content-basesd：</h4><p>基础CB推荐算法利用物品的基本信息和用户偏好内容的相似性进行物品推荐。通过分析用户已经浏览过的物品内容，生成用户的偏好内容，然后推荐与用户感兴趣的物品内容相似度高的其他物品。</p>
<p>比如，用户近期浏览过冯小刚导演的电影“非诚勿扰”，主演是葛优；那么如果用户没有看过“私人订制”，则可以推荐给用户。因为这两部电影的导演都是冯小刚，主演都有葛优。</p>
<p><strong>适用场景</strong></p>
<p>适用于基础CB架构的搭建，尤其是对新上线物品会马上被推荐非常有效，被推荐的机会与老的物品是相同的。</p>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-29/30285749.jpg" alt=""><br>ui表用户，qi表物品，uik表示用户在第 k 个方面的特征，qik表示物品在第 k 个方面的特征，<br>sim（uik，qik）表示ui,qi在第 k 个特征方面上的相似度<br><strong>算法流程</strong></p>
<p>算法输入：物品信息，用户行为日志。<br>算法输出：初始推荐结果。</p>
<ul>
<li>A. 物品表示：每个物品使用特征向量<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-29/87010752.jpg" alt=""></li>
<li>B. 从用户行为日志中，获取该用户所浏览、收藏、评价、分享的物品集合M，根据物品集合M中物品的特征数据，可以学到用户的内容偏好； </li>
<li>C. 保存Top-K个物品到初始推荐结果中。</li>
</ul>
<p><strong>适用场景</strong><br>适用于基础CB架构的搭建，尤其是对新上线物品会马上被推荐非常有效，被推荐的机会与老的物品是相同的。</p>
<h5 id="基于KNN的CB推荐算法"><a href="#基于KNN的CB推荐算法" class="headerlink" title="基于KNN的CB推荐算法"></a>基于KNN的CB推荐算法</h5><p>KNN(k-Nearest Neighbor)算法基于这样的假设：如果在特征空间中，一个样本的k个最邻近样本中的大多数样本属于某一个类别，则该样本也属于这个类别。<br>KNN在CB推荐算法中的应用于在CF推荐算法中的应用极为相似，它们都是要首先找到与目标物品相似的且已经被用户 u 评价过的 k 个物品，然后根据用户 u 对这 k 个物品的评价来预测其目标物品的评价。它们的差别在于，CF推荐算法中的KNN是根据用户对物品的评分来计算物品间相似度的，而CB推荐算法中KNN是根据物品画像来计算相似度的，所以对于后者来说，如何通过物品画像来计算物品间的相似度是算法中的关键步骤。相似度的计算可以使用余弦相似度或Pearson相关系数的计算方法。</p>
<p>算法输入：用户已评分物品，目标物品 i 。<br>算法输出：用户对目标物品 i 的评分。</p>
<h5 id="基于决策树的CB推荐算法"><a href="#基于决策树的CB推荐算法" class="headerlink" title="基于决策树的CB推荐算法"></a>基于决策树的CB推荐算法</h5><p>基于决策树的推荐算法在训练阶段会生成一个显示的决策模型。决策树可以通过训练数据构建并有效判断一个新的物品是否可能受到欢迎。当物品的特征属性较少时，采用决策树算法能够取得不错的效果，另外，决策树学习的思想也比较容易被理解，在物品推荐时的可解释性较好</p>
<p>在物品推荐系统中，决策树的内部节点通常表示物品的特征属性，这些节点用于区分物品集合，例如，通过物品中是否包含这个特征将其进行分类。在只有两个分类的简单数据集中，用户是否对物品感兴趣一般出现在决策树的叶子节点上。</p>
<h5 id="基于朴素贝叶斯的CB推荐算法"><a href="#基于朴素贝叶斯的CB推荐算法" class="headerlink" title="基于朴素贝叶斯的CB推荐算法"></a>基于朴素贝叶斯的CB推荐算法</h5><p>基于朴素贝叶斯的推荐系统假设用户和物品的特征向量中的各个分量之间条件独立，判断用户是否对某个物品有兴趣的方法是将这个问题转化为分类问题：喜欢和不喜欢。</p>
<h4 id="混合推荐算法"><a href="#混合推荐算法" class="headerlink" title="混合推荐算法"></a>混合推荐算法</h4><p>各种推荐方法都有优缺点，为了扬长补短，在实际中常常采用混合推荐（Hybrid Recommendation）。研究和应用最多的是内容推荐和协同过滤推荐的组合。最简单的做法就是分别用基于内容的方法和协同过滤推荐方法去产生一个推荐预测结果，然后用某方法组合其结果。尽管从理论上有很多种推荐组合方法，但在某一具体问题中并不见得都有效，组合推荐一个最重要原则就是通过组合后要能避免或弥补各自推荐技术的弱点。</p>
<h4 id="推荐系统的评估（Evaluation）"><a href="#推荐系统的评估（Evaluation）" class="headerlink" title="推荐系统的评估（Evaluation）"></a>推荐系统的评估（Evaluation）</h4><p>一个完整的推荐系统一般存在3个参与方：</p>
<ul>
<li>用户</li>
<li>物品提供者</li>
<li>提供推荐系统的网站</li>
</ul>
<p>好的推荐系统设计，能够让推荐系统本身收集到高质量的用户反馈，不断完善推荐的质量，增加用户和网站的交互，提高网站的收入。因此在评测一个推荐算法时，需要同时考虑三方的利益，一个好的推荐系统是能够令三方共赢的系统。</p>
<h5 id="推荐系统实验方法"><a href="#推荐系统实验方法" class="headerlink" title="推荐系统实验方法"></a>推荐系统实验方法</h5><p>一般来说，一个新的推荐算法最终上线，需要完成上面所说的3个实验：离线实验、用户调查和在线实验。</p>
<ul>
<li><p>离线实验的方法一般由如下几个步骤构成：</p>
<ul>
<li>通过日志系统获得用户行为数据，并按照一定格式生成一个标准的数据集；</li>
<li>将数据集按照一定的规则分成训练集和测试集；</li>
<li>在训练集上训练用户兴趣模型，在测试集上进行预测；</li>
<li><p>通过事先定义的离线指标评测算法在测试集上的预测结果。</p>
<p>从上面的步骤可以看到，推荐系统的离线实验都是在数据集上完成的，也就是说它不需要一个实际的系统来供它实验，而只要有一个从实际系统日志中提取的数据集即可，因此离线实验速度快，可以测试大量算法，这是离线实验的显著优点。而它的主要缺点是无法获得很多商业上关注的指标，如点击率、转化率等，而找到和商业指标非常相关的离线指标也是很困难的事情。</p>
</li>
</ul>
</li>
<li><p>用户调查</p>
<p>  离线实验的指标和实际的商业指标存在差距，比如预测准确率和用户满意度之间就存在很大差别，高预测准确率不等于高用户满意度。因此，如果要准确评测一个算法，需要相对比较真实的环境。最好的方法就是将算法直接上线测试，但在对算法会不会降低用户满意度不太有把握的情况下，上线测试具有较高的风险，所以在上线测试前一般需要做一次称为用户调查的测试。</p>
<p>  测试用户的选择必须尽量保证测试用户的分布和真实用户的分布相同，比如男女各半，以及年龄、活跃度的分布都和真实用户分布尽量相同。此外，用户调查要尽量保证是双盲实验，不要让实验人员和用户事先知道测试的目标，以免用户的回答和实验人员的测试受主观成分的影响。</p>
<p>  用户调查的优缺点也很明显。它的优点是可以获得很多体现用户主观感受的指标，相对在线实验风险很低，出现错误后很容易弥补。缺点是招募测试用户代价较大，很难组织大规模的测试用户，因此会使测试结果的统计意义不足。此外，在很多时候设计双盲实验非常困难，而且用户在测试环境下的行为和真实环境下的行为可能有所不同，因而在测试环境下收集的测试指标可能在真实环境下无法重现。</p>
</li>
<li><p>在线实验<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-29/7521046.jpg" alt=""><br>在完成离线实验和必要的用户调查后，可以将推荐系统上线做AB测试，将它和旧的算法进行比较。<strong>AB测试是一种很常用的在线评测算法的实验方法。</strong>它通过一定的规则将用户随机分成几组，并对不同组的用户采用不同的算法，然后通过统计不同组用户的各种不同的评测指标比较不同算法，比如可以统计不同组用户的点击率，通过点击率比较不同算法的性能。对AB测试感兴趣的读者可以浏览一下网站 <a href="http://www.abtests.com/，" target="_blank" rel="noopener">http://www.abtests.com/，</a> 该网站给出了很多通过实际AB测试提高网站用户满意度的例子，从中我们可以学习到如何进行合理的AB测试。</p>
</li>
</ul>
<p>AB测试的优点是可以公平获得不同算法实际在线时的性能指标，包括商业上关注的指标。AB测试的缺点主要是周期比较长，必须进行长期的实验才能得到可靠的结果。因此一般不会用AB测试测试所有的算法，而只是用它测试那些在离线实验和用户调查中表现很好的算法。其次，一个大型网站的AB测试系统的设计也是一项复杂的工程。一个大型网站的架构分前端和后端，从前端展示给用户的界面到最后端的算法，中间往往经过了很多层，这些层往往由不同的团队控制，而且都有可能做AB测试。</p>
<p>如果为不同的层分别设计AB测试系统，那么不同的AB测试之间往往会互相干扰。比如，当我们进行一个后台推荐算法的AB测试，同时网页团队在做推荐页面的界面AB测试，最终的结果就是你不知道测试结果是自己算法的改变，还是推荐界面的改变造成的。因此，切分流量是AB测试中的关键，不同的层以及控制这些层的团队需要从一个统一的地方获得自己AB测试的流量，而不同层之间的流量应该是正交的。</p>
<h4 id="评测指标"><a href="#评测指标" class="headerlink" title="评测指标"></a>评测指标</h4><ul>
<li><p>预测准确度</p>
<p>  预测准确度衡量一个推荐系统或者推荐算法预测用户行为的能力，是最重要的推荐系统离线评测指标。从推荐系统诞生的那一天起，几乎99%与推荐相关的论文都在讨论这个指标。这主要是因为该指标可以通过离线实验计算，方便了很多学术界的研究人员研究推荐算法。</p>
</li>
</ul>
<ul>
<li><p>TopN推荐</p>
<p>网站在提供推荐服务时，一般是给用户一个个性化的推荐列表，这种推荐叫做TopN推荐。TopN推荐的预测准确率一般通过准确率（precision）/召回率（recall）度量。令 R(u) 是根据用户在训练集上的行为给用户作出的推荐列表，而 T(u) 是用户在测试集上的行为列表。那么，推荐结果的召回率定义为：</p>
<p>  <img src="http://phal1xgub.bkt.clouddn.com/18-10-29/29026798.jpg" alt=""></p>
<p>  推荐结果的精准率定义为：</p>
<p>  <img src="http://phal1xgub.bkt.clouddn.com/18-10-29/90256800.jpg" alt=""></p>
</li>
<li><p>覆盖率 </p>
<p>  覆盖率（coverage）描述一个推荐系统对物品长尾的发掘能力。覆盖率有不同的定义方法，最简单的定义为推荐系统能够推荐出来的物品占总物品集合的比例。假设系统的用户集合为 U，推荐系统给每个用户推荐一个长度为N的物品列表R(u)。那么推荐系统的覆盖率可以通过下面的公式计算： </p>
<p>  <img src="http://phal1xgub.bkt.clouddn.com/18-10-29/37232195.jpg" alt=""></p>
<p>  覆盖率是一个内容提供商会关心的指标。以图书推荐为例，出版社可能会很关心他们的书有没有被推荐给用户。覆盖率为100%的推荐系统可以将每个物品都推荐给至少一个用户。此外，从上面的定义也可以看到，热门排行榜的推荐覆盖率是很低的，它只会推荐那些热门的物品，这些物品在总物品中占的比例很小。一个好的推荐系统不仅需要有比较高的用户满意度，也要有较高的覆盖率。</p>
</li>
<li><p>多样性</p>
<p>  为了满足用户广泛的兴趣，推荐列表需要能够覆盖用户不同的兴趣领域，即推荐结果需要具有多样性。尽管用户的兴趣在较长的时间跨度中是一样的，但具体到用户访问推荐系统的某一刻，其兴趣往往是单一的，那么如果推荐列表只能覆盖用户的一个兴趣点，而这个兴趣点不是用户这个时刻的兴趣点，推荐列表就不会让用户满意。反之，如果推荐列表比较多样，覆盖了用户绝大多数的兴趣点，那么就会增加用户找到感兴趣物品的概率。因此给用户的推荐列表也需要满足用户广泛的兴趣，即具有多样性。<br>  <strong>多样性描述了推荐列表中物品两两之间的不相似性。因此，多样性和相似性是对应的。</strong></p>
</li>
<li><p>新颖性<br>新颖的推荐是指给用户推荐那些他们以前没有听说过的物品。在一个网站中实现新颖性的最简单办法是，把那些用户之前在网站中对其有过行为的物品从推荐列表中过滤掉。比如在一个视频网站中，新颖的推荐不应该给用户推荐那些他们已经看过、打过分或者浏览过的视频。但是，有些视频可能是用户在别的网站看过，或者是在电视上看过，因此仅仅过滤掉本网站中用户有过行为的物品还不能完全实现新颖性。</p>
<p>  但是，用推荐结果的平均流行度度量新颖性比较粗略，因为不同用户不知道的东西是不同的。因此，要准确地统计新颖性需要做用户调查。</p>
</li>
<li><p>惊喜度</p>
<p>  惊喜度（serendipity）是最近这几年推荐系统领域最热门的话题。令用户惊喜的推荐结果是和用户历史上喜欢的物品不相似，但用户却觉得满意的推荐。那么，定义惊喜度需要首先定义推荐结果和用户历史上喜欢的物品的相似度，其次需要定义用户对推荐结果的满意度。</p>
</li>
</ul>
<pre><code>用户满意度只能通过问卷调查或者在线实验获得，而推荐结果和用户历史上喜欢的物品相似度一般可以用内容相似度定义。也就是说，如果获得了一个用户观看电影的历史，得到这些电影的演员和导演集合A，然后给用户推荐一个不属于集合A的导演和演员创作的电影，而用户表示非常满意，这样就实现了一个惊喜度很高的推荐。因此提高推荐惊喜度需要提高推荐结果的用户满意度，同时降低推荐结果和用户历史兴趣的相似度。
</code></pre><h4 id="推荐系统的冷启动问题（Cold-Start）"><a href="#推荐系统的冷启动问题（Cold-Start）" class="headerlink" title="推荐系统的冷启动问题（Cold Start）"></a>推荐系统的冷启动问题（Cold Start）</h4><p>推荐系统需要根据用户的历史行为和兴趣预测用户未来的行为和兴趣，对于BAT这类大公司来说，它们已经积累了大量的用户数据，不发愁。但是对于很多做纯粹推荐系统的网站或者很多在开始阶段就希望有个性化推荐应用的网站来说，如何在对用户一无所知（即没有用户行为数据）的情况下进行最有效的推荐呢？这就衍生了冷启动问题。</p>
<p><strong>冷启动问题主要分为3类：</strong></p>
<ul>
<li>用户冷启动，即如何给新用户做个性化推荐</li>
<li>物品冷启动，即如何将新的物品推荐给可能对它感兴趣的用户</li>
<li>系统冷启动，即如何在一个新开发的网站（没有用户，没有用户行为，只有部分物品信息）上设计个性化推荐系统，从而在网站刚发布时就让用户体会到个性化推荐</li>
</ul>
<p><strong>冷启动问题的解决方案</strong></p>
<ul>
<li>提供非个性化的推荐<br>最简单的例子就是提供热门排行榜，可以给用户推荐热门排行榜，等到用户数据收集到一定的时候，再切换为个性化推荐。Netflix的研究也表明新用户在冷启动阶段确实是更倾向于热门排行榜的，老用户会更加需要长尾推荐。</li>
<li><p>利用用户注册信息</p>
<p>  用户的注册信息主要分为3种：<br>  人口统计学信息，包括年龄、性别、职业、民族、学历和居住地<br>  用户兴趣的描述，部分网站会让用户用文字来描述兴趣<br>  从其他网站导入的用户站外行为，比如用户利用社交网站账号登录，就可以在获得用户授权的情况下导入用户在该社交网站的部分行为数据和社交网络数据。</p>
<p>  这种个性化的粒度很粗，假设性别作为一个粒度来推荐，那么所有刚注册的女性看到的都是同样的结果，但是相对于男女不区分的方式，这种推荐精度已经大大提高了。</p>
</li>
</ul>
<ul>
<li><p>选择合适的物品启动用户的兴趣</p>
<p>  用户在登录时对一些物品进行反馈，收集用户对这些物品的兴趣信息，然后给用户推荐那些和这些物品相似的物品。一般来说，能够用来启动用户兴趣的物品需要具有以下特点：</p>
<p>  比较热门，如果要让用户对物品进行反馈，前提是用户得知道这是什么东西；<br>  具有代表性和区分性，启动用户兴趣的物品不能是大众化或老少咸宜的，因为这样的物品对用户的兴趣没有区分性；<br>  启动物品集合需要有多样性，在冷启动时，我们不知道用户的兴趣，而用户兴趣的可能性非常多，为了匹配多样的兴趣，我们需要提供具有很高覆盖率的启动物品集合，这些物品能覆盖几乎所有主流的用户兴趣。</p>
</li>
<li><p>利用物品的内容信息</p>
<p>  物品冷启动问题在新闻网站等时效性很强的网站中非常重要，因为这些网站时时刻刻都有新物品加入，而且每个物品必须能够再第一时间展现给用户，否则经过一段时间后，物品的价值就大大降低了。</p>
</li>
</ul>
<pre><code>对UserCF算法来说，针对推荐列表并不是给用户展示内容的唯一列表（大多网站都是这样的）的网站。当新物品加入时，总会有用户通过某些途径看到，那么当一个用户对其产生反馈后，和他历史兴趣相似的用户的推荐列表中就有可能出现该物品，从而更多的人对该物品做出反馈，导致更多的人的推荐列表中出现该物品。



因此，该物品就能不断扩散开来，从而逐步展示到对它感兴趣用户的推荐列表中针对推荐列表是用户获取信息的主要途径（例如豆瓣网络电台）的网站UserCF算法就需要解决第一推动力的问题，即第一个用户从哪儿发现新物品。最简单的方法是将新的物品随机战士给用户，但是太不个性化。因此可以考虑利用物品的内容信息，将新物品先投放给曾经喜欢过和它内容相似的其他物品的用户。



对ItemCF算法来说，物品冷启动就是很严重的问题了。因为该算法的基础是通过用户对物品产生的行为来计算物品之间的相似度，当新物品还未展示给用户时，用户就无法产生行为。为此，只能利用物品的内容信息计算物品的相关程度。基本思路就是将物品转换成关键词向量，通过计算向量之间的相似度（例如计算余弦相似度），得到物品的相关程度。
</code></pre><ul>
<li><p>采用专家标注</p>
<p>  很多系统在建立的时候，既没有用户的行为数据，也没有充足的物品内容信息来计算物品相似度。这种情况下，很多系统都利用专家进行标注。</p>
</li>
</ul>
<pre><code>代表系统：个性化网络电台Pandora、电影推荐网站Jinni。



以Pandora电台为例，Pandora雇用了一批音乐人对几万名歌手的歌曲进行各个维度的标注，最终选定了400多个特征。每首歌都可以标识为一个400维的向量，然后通过常见的向量相似度算法计算出歌曲的相似度。
</code></pre><h4 id="推荐系统学术研究常用数据集"><a href="#推荐系统学术研究常用数据集" class="headerlink" title="推荐系统学术研究常用数据集"></a>推荐系统学术研究常用数据集</h4><ul>
<li><p>MovieLen(<a href="https://grouplens.org/datasets/movielens/" target="_blank" rel="noopener">https://grouplens.org/datasets/movielens/</a>)</p>
<p>  MovieLens数据集中，用户对自己看过的电影进行评分，分值为1~5。MovieLens包括两个不同大小的库，适用于不同规模的算法。小规模的库是943个独立用户对1 682部电影作的10 000次评分的数据；大规模的库是6 040个独立用户对3 900部电影作的大约100万次评分。</p>
</li>
<li><p>BookCrossing(<a href="http://www2.informatik.uni-freiburg.de/~cziegler/BX/" target="_blank" rel="noopener">http://www2.informatik.uni-freiburg.de/~cziegler/BX/</a>) </p>
<p>  这个数据集是网上的Book-Crossing图书社区的278858个用户对271379本书进行的评分，包括显式和隐式的评分。这些用户的年龄等人口统计学属性(demographic feature)都以匿名的形式保存并供分析。这个数据集是由Cai-Nicolas Ziegler使用爬虫程序在2004年从Book-Crossing图书社区上采集的。</p>
</li>
<li><p>Jester Joke(<a href="http://eigentaste.berkeley.edu/dataset/" target="_blank" rel="noopener">http://eigentaste.berkeley.edu/dataset/</a>) </p>
</li>
</ul>
<p>Jester Joke是一个网上推荐和分享笑话的网站。这个数据集有73496个用户对100个笑话作的410万次评分。评分范围是−10~10的连续实数。这些数据是由加州大学伯克利分校的Ken Goldberg公布的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mobyIsMe.github.io/2018/10/10/推荐架构/" data-id="cjxxhv3ur000ak2r00qc2biof" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-kaggle-titanic" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/10/kaggle-titanic/" class="article-date">
  <time datetime="2018-10-10T08:44:43.000Z" itemprop="datePublished">2018-10-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/10/kaggle-titanic/">kaggle-titanic</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>kaggle上一些数据处理的经验：</p>
<blockquote>
</blockquote>
<p>『对数据的认识太重要了！』<br>『数据中的特殊点/离群点的分析和处理太重要了！』<br>『特征工程(feature engineering)太重要了！在很多Kaggle的场景下，甚至比model本身还要重要』<br>『要做模型融合(model ensemble)啊啊啊！』</p>
<h5 id="工作流程"><a href="#工作流程" class="headerlink" title="工作流程"></a>工作流程</h5><ol>
<li>问题定义Question or problem definition.</li>
<li>获取训练和测试数据Acquire training and testing data.</li>
<li>数据准备和清洗Wrangle, prepare, cleanse the data.</li>
<li>分析，识别数据模型，探索数据Analyze, identify patterns, and explore the data.</li>
<li>建模，预测，解决问题Model, predict and solve the problem.</li>
<li>可视化，报表，展示解决步骤和最终解决方案Visualize, report, and present the problem solving steps and final solution.</li>
<li>提交结果Supply or submit the results.</li>
</ol>
<h5 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h5><ul>
<li>Classifying：样本分类或分级</li>
<li>Correlating：样本预测结果和特征的关联程度，特征之间的关联程度</li>
<li>Converting：特征转换（向量化）</li>
<li>Completing：特征缺失值预估完善</li>
<li>Correcting：对于明显离群或会造成预测结果明显倾斜的异常数据，进行修正或排除</li>
<li>Creating：根据现有特征衍生新的特征，以满足关联性、向量化以及完整度等目标上的要求</li>
<li>Charting：根据数据性质和问题目标选择正确的可视化图表</li>
</ul>
<h5 id="初始数据"><a href="#初始数据" class="headerlink" title="初始数据"></a>初始数据</h5><p>在Data下有train.csv和test.csv两个文件，分别存着官方给的训练和测试数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd #数据分析</span><br><span class="line">import numpy as np #科学计算</span><br><span class="line">from pandas import Series,DataFrame</span><br><span class="line"></span><br><span class="line">data_train = pd.read_csv(&quot;/Users/Hanxiaoyang/Titanic_data/Train.csv&quot;)</span><br><span class="line">data_train.head()</span><br></pre></td></tr></table></figure>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/13834077.jpg" alt=""></p>
<p>总共有12列，其中Survived字段表示的是该乘客是否获救，其余都是乘客的个人信息，包括：</p>
<blockquote>
</blockquote>
<p>PassengerId: 乘客ID<br>Pclass ： 乘客等级(1/2/3等舱位)<br>Name ：乘客姓名<br>Sex ：性别<br>Age ：年龄<br>SibSp ：堂兄弟/妹个数<br>Parch ：父母与小孩个数<br>Ticket ：船票信息<br>Fare ：票价<br>Cabin ：客舱<br>Embarked ：登船港口</p>
<p>查看数据的摘要信息：<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/43441627.jpg" alt=""></p>
<p>查看非数值类（object类型）特征的数据分布情况<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/87098057.jpg" alt=""></p>
<h5 id="初步分析"><a href="#初步分析" class="headerlink" title="初步分析"></a>初步分析</h5><p>看看各个属性和最后的Survived之间的关系。</p>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/2753343.jpg" alt=""></p>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/70004910.jpg" alt=""></p>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/50833042.jpg" alt=""></p>
<p>年龄这类跨度较长的特征使用直方图分别查看生还与否的分布<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/72825576.jpg" alt=""></p>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/77836464.jpg" alt=""></p>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/52884143.jpg" alt=""><br>不同登录港口对幸存率的影响<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/99259876.jpg" alt=""></p>
<p>可以看出：年龄、性别、几等仓位、是否有兄弟姐妹都对幸存率有很相关的影响</p>
<h5 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h5><h6 id="缺失值处理："><a href="#缺失值处理：" class="headerlink" title="缺失值处理："></a>缺失值处理：</h6><p>通常遇到缺值的情况，我们会有几种常见的处理方式</p>
<ul>
<li>如果缺值的样本占总数比例极高，我们可能就直接舍弃了，作为特征加入的话，可能反倒带入noise，影响最后的结果了</li>
<li>如果缺值的样本适中，而该属性非连续值特征属性(比如说类目属性)，那就把NaN作为一个新类别，加到类别特征中</li>
<li>如果缺值的样本适中，而该属性为连续值特征属性，有时候我们会考虑给定一个step(比如这里的age，我们可以考虑每隔2/3岁为一个步长)，然后把它离散化，之后把NaN作为一个type加到属性类目中。</li>
<li>有些情况下，缺失的值个数并不是特别多，那我们也可以试着根据已有的值，拟合一下数据，补充上。<br>由于年龄属性有缺失，所以先采用中位数填充：</li>
</ul>
<p>查看缺失值:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(traindata.isnull().any())</span><br><span class="line">print(testdata.isnull().any())</span><br></pre></td></tr></table></figure>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/80562408.jpg" alt=""></p>
<p>PassengerId就是个ID号，<br>Survived是否生存的标记，0，1<br>Pclass票价等级，1、2、3<br>Name姓名，为了简单起见，我们先不使用这个特征<br>Sex，性别，我们转换为0、1（乘客男的更加多点）</p>
<p>Age年龄，train和test集都有很多缺失值，我们使用中位数来替换缺失值（train和test集的分布都差不多）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 给age年龄字段的空值填充估值</span><br><span class="line"># 使用相同Pclass和Title的Age中位数来替代（对于中位数为空的组合，使用Title整体的中位数来替代）</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for dataset in combine:</span><br><span class="line">    # 取6种组合的中位数</span><br><span class="line">    for i in range(0, 6):</span><br><span class="line">        </span><br><span class="line">        for j in range(0, 3):</span><br><span class="line">            guess_title_df = dataset[dataset[&quot;Title&quot;]==i+1][&quot;Age&quot;].dropna()</span><br><span class="line">            </span><br><span class="line">            guess_df = dataset[(dataset[&apos;Title&apos;] == i+1) &amp; (dataset[&apos;Pclass&apos;] == j+1)][&apos;Age&apos;].dropna()</span><br><span class="line">            </span><br><span class="line">            # age_mean = guess_df.mean()</span><br><span class="line">            # age_std = guess_df.std()</span><br><span class="line">            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)</span><br><span class="line"></span><br><span class="line">            age_guess = guess_df.median() if ~np.isnan(guess_df.median()) else guess_title_df.median()</span><br><span class="line">            #print(i,j,guess_df.median(),guess_title_df.median(),age_guess)</span><br><span class="line">            # Convert random age float to nearest .5 age</span><br><span class="line">            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5</span><br><span class="line">    # 给满足6中情况的Age字段赋值</span><br><span class="line">    for i in range(0, 6):</span><br><span class="line">        for j in range(0, 3):</span><br><span class="line">            dataset.loc[ (dataset.Age.isnull()) &amp; (dataset.Title == i+1) &amp; (dataset.Pclass == j+1),</span><br><span class="line">                        &apos;Age&apos;] = guess_ages[i,j]</span><br><span class="line"></span><br><span class="line">    dataset[&apos;Age&apos;] = dataset[&apos;Age&apos;].astype(int)</span><br><span class="line"></span><br><span class="line">train_df.head()</span><br></pre></td></tr></table></figure>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/53710620.jpg" alt=""></p>
<h6 id="非数值特征值离散化："><a href="#非数值特征值离散化：" class="headerlink" title="非数值特征值离散化："></a>非数值特征值离散化：</h6><p>cabin[yes,no],embarked[s,c,o],sex[male,female]<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/19597981.jpg" alt=""></p>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/31857525.jpg" alt=""><br>是否有cabin<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/48879195.jpg" alt=""></p>
<h6 id="特征构建"><a href="#特征构建" class="headerlink" title="特征构建"></a>特征构建</h6><p>将parch（父母与小孩个数）和sibsip（堂兄弟/妹个数）组合为家庭规模familysize特征，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 创建家庭规模FamilySize组合特征</span><br><span class="line">for dataset in combine:</span><br><span class="line">    dataset[&quot;FamilySize&quot;] = dataset[&quot;Parch&quot;] + dataset[&quot;SibSp&quot;] + 1</span><br><span class="line">train_df[[&quot;FamilySize&quot;,&quot;Survived&quot;]].groupby([&quot;FamilySize&quot;],as_index = False).mean().sort_values(by=&quot;FamilySize&quot;,ascending=True)</span><br></pre></td></tr></table></figure>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/23344260.jpg" alt=""></p>
<p>将fare票价特征创建区间特征:<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/42686935.jpg" alt=""></p>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/64383745.jpg" alt=""></p>
<p>另外，如果属性的数值幅度变化过大，在逻辑回归与梯度下降的话，会知道，各属性值之间scale差距太大，将对收敛速度造成几万点伤害值！甚至不收敛！ 所以使用scikit-learn里面的preprocessing模块对数值范围过大的属性做一个scaling，所谓scaling，其实就是将一些变化幅度较大的特征化到[-1,1]之内。</p>
<p>特征工程完成后如下：</p>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/74720666.jpg" alt=""></p>
<p>特征相关性可视化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 用seaborn的heatmap对特征之间的相关性进行可视化</span><br><span class="line">colormap = plt.cm.RdBu</span><br><span class="line">plt.figure(figsize=(14,12))</span><br><span class="line">plt.title(&apos;Pearson Correlation of Features&apos;, y=1.05, size=15)</span><br><span class="line">sns.heatmap(train_df.astype(float).corr(),linewidths=0.1,vmax=1.0, </span><br><span class="line">            square=True, cmap=colormap, linecolor=&apos;white&apos;, annot=True)</span><br></pre></td></tr></table></figure>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/71290880.jpg" alt=""></p>
<h5 id="建模和优化"><a href="#建模和优化" class="headerlink" title="建模和优化"></a>建模和优化</h5><h6 id="数据集准备："><a href="#数据集准备：" class="headerlink" title="数据集准备："></a>数据集准备：</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cross_validation import train_test_split</span><br><span class="line">X_train = train_df.drop(&quot;Survived&quot;,axis=1)</span><br><span class="line">Y_train = train_df[&quot;Survived&quot;]</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.3, random_state=0)</span><br></pre></td></tr></table></figure>
<p>模型比较:</p>
<ul>
<li>LR:</li>
</ul>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/79209102.jpg" alt=""></p>
<ul>
<li><p>SVM：<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/45867680.jpg" alt=""></p>
</li>
<li><p>决策树：<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/81603825.jpg" alt=""></p>
</li>
<li><p>朴素贝叶斯:</p>
</li>
</ul>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/89611435.jpg" alt=""></p>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结:"></a>总结:</h5><p>  可以优化的点：</p>
<ul>
<li>Age属性不使用现在的拟合方式，而是根据名称中的『Mr』『Mrs』『Miss』等的平均值进行填充。</li>
<li>Age不做成一个连续值属性，而是使用一个步长进行离散化，变成离散的类目feature。</li>
<li>Cabin再细化一些，对于有记录的Cabin属性，我们将其分为前面的字母部分(我猜是位置和船层之类的信息) 和 后面的数字部分(应该是房间号，有意思的事情是，如果你仔细看看原始数据，你会发现，这个值大的情况下，似乎获救的可能性高一些)。</li>
<li>Pclass和Sex俩太重要了，我们试着用它们去组出一个组合属性来试试，这也是另外一种程度的细化。</li>
<li>单加一个Child字段，Age&lt;=12的，设为1，其余为0(你去看看数据，确实小盆友优先程度很高啊)</li>
<li>如果名字里面有『Mrs』，而Parch&gt;1的，我们猜测她可能是一个母亲，应该获救的概率也会提高，因此可以多加一个Mother字段，此种情况下设为1，其余情况下设为0</li>
<li>登船港口可以考虑先去掉试试(Q和C本来就没权重，S有点诡异)</li>
<li>把堂兄弟/兄妹 和 Parch 还有自己 个数加在一起组一个Family_size字段(考虑到大家族可能对最后的结果有影响)</li>
<li>Name是一个我们一直没有触碰的属性，我们可以做一些简单的处理，比如说男性中带某些字眼的(‘Capt’, ‘Don’, ‘Major’, ‘Sir’)可以统一到一个Title，女性也一样。</li>
<li>模型融合：模型融合可以比较好地缓解，训练过程中产生的过拟合问题，从而对于结果的准确度提升有一定的帮助。当我们手头上有一堆在同一份数据集上训练得到的分类器(比如logistic regression，SVM，KNN，random forest，神经网络)，那我们让他们都分别去做判定，然后对结果做投票统计，取票数最多的结果为最后结果。</li>
</ul>
<p>整个流程为:<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/86373785.jpg" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mobyIsMe.github.io/2018/10/10/kaggle-titanic/" data-id="cjxxhv3tr0004k2r0yhhbjb45" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-movielens" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/10/01/movielens/" class="article-date">
  <time datetime="2018-10-01T11:02:57.000Z" itemprop="datePublished">2018-10-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/10/01/movielens/">movielens</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>两种最普遍的推荐系统的类型是<strong>基于内容</strong>和<strong>协同过滤（CF</strong>）。协同过滤基于用户对产品的态度产生推荐，也就是说，它使用“人群的智慧”来推荐产品。与此相反，基于内容的推荐系统集中于物品的属性，并基于它们之间的相似性为你推荐。一般情况下，协作过滤（CF）是推荐引擎的主力。该算法具有能够自身进行特征学习的一个非常有趣的特性，这意味着它可以开始学习使用哪些特性。<strong>CF可以分为基于内存的协同过滤和基于模型的协同过滤。</strong>在本教程中，你将使用奇异值分解（SVD）实现基于模型的CF和通过计算余弦相似实现基于内存的CF。</p>
<p>当你有一个非常稀疏的多维矩阵时，通过进行矩阵分解可以调整用户-产品矩阵为低等级的结构，然后你可以通过两个低秩矩阵（其中，每行包含该本征矢量）的乘积来代表该矩阵。你通过将低秩矩阵相乘，在原始矩阵填补缺少项，以调整这个矩阵，从而尽可能的近似原始矩阵。</p>
<p>基于模型的协同过滤是基于矩阵分解（MF），主要使用SVD来分解矩阵，它已获得更大的曝光，它主要是作为潜变量分解和降维的一个无监督学习方法。矩阵分解广泛用于推荐系统，其中，它比基于内存的CF可以更好地处理与扩展性和稀疏性. MF的目标是从已知的评分中学习用户的潜在喜好和产品的潜在属性（学习描述评分特征的特征），随后通过用户和产品的潜在特征的点积预测未知的评分。如果只有少数可用的数据，那么基于模型的CF模式将预测不良，因为这将更难以学习潜在特征。</p>
<p>基于内存的CF的缺点是，它不能扩展到真实世界的场景，并且没有解决众所周知的冷启动问题，也就是当新用户或新产品进入系统时。基于模型的CF方法是可扩展的，并且可以比基于内存的模型处理更高的稀疏度，但当没有任何评分的用户或产品进入系统时，也是苦不堪言的。</p>
<p>同时使用评分和内容特性的模型称为混合推荐系统，其中，协同过滤和基于内容的模型相结合。混合推荐系统通常比协同过滤或基于内容的模型自身表现出更高的精度：它们有能力更好的解决冷启动问题，因为如果你没有一个用户或者一个产品的评分，那么你可以使用该用户或产品的元数据进行预测。混合推荐系统将在未来的教程中介绍。</p>
<p>推荐系统的重要算法是协同过滤，movielens是很成熟的开源数据库，自己在这个数据库上分别使用了user-cf和item-cf分别进行尝试，并在多个评价维度指标上进行对比。</p>
<h4 id="推荐理论"><a href="#推荐理论" class="headerlink" title="推荐理论:"></a>推荐理论:</h4><p>分为两个步骤，第一：找到目标之间的相似性  第二：根据相似性去推荐<br>推荐最基本的方法就是基于邻域的算法——协同过滤，Collaborative Filtering，CF(还有隐语义模型，基于图的模型)</p>
<ul>
<li><p>user-based: 对用户A进行推荐 ，按照上面两个步骤就是，</p>
<pre><code>找到和目标用户兴趣相似的用户集合。
找到这个集合中的用户喜欢的，且目标用户没有听说过的物品推荐给目标用户。
</code></pre><ul>
<li><p>兴趣相似度计算：<br>这里最关键的就是计算用户的兴趣相似度： 主要利用行为相似度来计算兴趣相似度，3种方法：</p>
<ul>
<li>jaccard公式：</li>
<li>余弦公式：</li>
<li>皮尔逊公式：  将余弦公式的每个变量均值化就是皮尔森公式<br>皮尔森相关系数公式对变量进行了均值化（或去中心化）处理，其好处是减少变量个体的数值差异对变量间相似度的影响。举个例子，用户A习惯对所有物品的评分范围为[1,3]，而用户B习惯对所有物品的评分范围为[3,5]，因各自评分数值标准的差异，采用余弦夹角公式计算两者的相似度比较低，但实际上两者的相似度是很高的。因此，在实际的推荐系统中，大都采用皮尔森相关系数公式</li>
</ul>
</li>
</ul>
</li>
<li><p>item-based:例如推荐电影，推荐音乐：给用户推荐那些和他们之前喜欢的物品相似的物品.网站上（购买了该商品的用户也经常购买的其他商品) </p>
<ul>
<li>同样的也分为两步：<br>(1) 计算物品之间的相似度。<br>(2) 根据物品的相似度和用户的历史行为给用户生成推荐列表。 </li>
<li>优点：可以提供推荐理由解释，利用用户历史兴趣做推荐。</li>
</ul>
</li>
<li><p>usercf vs itemcf:<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/67060815.jpg" alt=""></p>
</li>
</ul>
<h4 id="MovieLens数据集"><a href="#MovieLens数据集" class="headerlink" title="MovieLens数据集"></a>MovieLens数据集</h4><p>用户对电影的评分。这个数据集有不同的版本，不同版本用户数和评分数都不一样，我采用了的100k这个数据集，该数据集记录了943个用户对1682部电影的共100,000个评分，每个用户至少对20部电影进行了评分。<br>文件u.item保存了item的信息，也就是电影的信息，共1682部电影，其id依次是1、2、……、1682。文件中每一行保存了一部电影的信息，格式如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">movie id | movie title | release date | video release date |</span><br><span class="line">IMDb URL | unknown | Action | Adventure | Animation |</span><br><span class="line">Children&apos;s | Comedy | Crime | Documentary | Drama | Fantasy |</span><br><span class="line">Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |</span><br><span class="line">Thriller | War | Western |</span><br></pre></td></tr></table></figure>
<p>注意，最后19个字段保存的是该电影的类型，一个字段对应一个类型，值为0代表不属于该类型，值为1代表属于该类型，类型信息保存在文件u.genre中。</p>
<p>文件u.user保存了用户的信息，共有943个用户，其id依次是1、2、……、943。文件中每一行保存了一个用户的信息，格式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">user id | age | gender | occupation | zip code</span><br></pre></td></tr></table></figure>
<p>文件u.occupation保存了用户职业的集合。<br>下面介绍数据集的主要文件。</p>
<p>文件u.data保存了所有的评分记录，每一行是一个用户对一部电影的评分，共有100000条记录。当然，如果某用户没有对某电影评分，则不会包含在该文件中。评分的分值在1到5之间，就是1、2、3、5这5个评分。每一行格式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">user id | item id | rating | timestamp</span><br></pre></td></tr></table></figure>
<p>其中，item id就是电影的id，时间戳timestamp是评分时间。</p>
<p>实验设计：协同过滤算法的离线实验一般如下设计。首先，将用户行为数据集按照均匀分布随机分成M 份(本章取M=8)，挑选一份作为测试集，将剩下的M-1份作为训练集。然后在训练集上建立用户 兴趣模型，并在测试集上对用户行为进行预测，统计出相应的评测指标。为了保证评测指标并不是过拟合的结果，需要进行M次实验，并且每次都使用不同的测试集。然后将M次实验测出的评 测指标的平均值作为最终的评测指标。 这样做主要是防止某次实验的结果是过拟合的结果(over fitting)，但如果数据集够大，模型 够简单，为了快速通过离线实验初步地选择算法，也可以只进行一次实验。 </p>
<h4 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现:"></a>算法实现:</h4><h5 id="user-cf部分"><a href="#user-cf部分" class="headerlink" title="user-cf部分:"></a>user-cf部分:</h5><p>计算两个用户的兴趣相似度，这里我们可以用余弦相似度来计算，即相似度 = 用户u和用户v共同评价过的电影数 / √(用户u评价的电影数 * 用户v评价的电影数)。由于对两两用户计算余弦相似度非常耗时，所以我们可以先计算这个公式的分子，如果分子为0，即用户u和用户v没有共同评价过的电影，则无需计算余弦相似度。为此我们可以建立电影到用户的倒排表，对于每部电影都保存对评价过该电影的用户列表，然后用稀疏矩阵usersim_mat表示用户u和用户v共同评价过的电影，这样子，扫描一遍倒排表并将同一物品下的两两用户对应的矩阵值加1，就可以通过稀疏矩阵中值为0的点知道哪些用户没有共同评价的电影了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def calc_user_sim(self):</span><br><span class="line">	# 构建物品-用户倒排表</span><br><span class="line">	movie2users = dict()</span><br><span class="line">	for user, movies in self.trainset.items():</span><br><span class="line">		for movie in movies:</span><br><span class="line">			if movie not in movie2users:</span><br><span class="line">				movie2users[movie] = set()</span><br><span class="line">			movie2users[movie].add(user)</span><br><span class="line">			if movie not in self.movie_popular:</span><br><span class="line">				self.movie_popular[movie] = 0</span><br><span class="line">			self.movie_popular[movie] += 1</span><br><span class="line"></span><br><span class="line">	# 计算两两用户之前的共同评价电影数</span><br><span class="line">	usersim_mat = self.user_sim_mat</span><br><span class="line">	for movie, users in movie2users.items():</span><br><span class="line">		for u in users:</span><br><span class="line">			for v in users:</span><br><span class="line">				if u == v:</span><br><span class="line">					continue</span><br><span class="line">				usersim_mat.setdefault(u, &#123;&#125;)</span><br><span class="line">				usersim_mat[u].setdefault(v, 0)</span><br><span class="line">				usersim_mat[u][v] += 1</span><br><span class="line"></span><br><span class="line">	# 计算用户兴趣相似度</span><br><span class="line">	for u, related_users in usersim_mat.items():</span><br><span class="line">		for v, count in related_users.items():</span><br><span class="line">			usersim_mat[u][v] = count / math.sqrt(len(self.trainset[u]) * len(self.trainset[v]))</span><br></pre></td></tr></table></figure>
<p>得到用户之间的兴趣相似度后，我们就可以用UserCF算法给用户推荐和他兴趣最相似的K个用户的电影了，这里我们直接用K个用户中看过某电影的用户群的兴趣相似度之和来表示被推荐用户对某部电影的感兴趣程度，具体的推荐函数实现如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def recommend(self, user):</span><br><span class="line">	K = self.n_sim_user</span><br><span class="line">	N = self.n_rec_movie</span><br><span class="line">	rank = dict()</span><br><span class="line">	watched_movies = self.trainset[user]</span><br><span class="line"></span><br><span class="line">	for similar_user, similarity_factor in sorted(self.user_sim_mat[user].items(),</span><br><span class="line">                                                  key=itemgetter(1), reverse=True)[0:K]:</span><br><span class="line">		for movie in self.trainset[similar_user]:</span><br><span class="line">			if movie in watched_movies:</span><br><span class="line">				continue</span><br><span class="line">			# 预测该用户对每部电影的兴趣</span><br><span class="line">			rank.setdefault(movie, 0)</span><br><span class="line">			rank[movie] += similarity_factor</span><br><span class="line">	# 返回评分最高的N部电影</span><br><span class="line">	return sorted(rank.items(), key=itemgetter(1), reverse=True)[0:N]</span><br></pre></td></tr></table></figure>
<h5 id="item-cf部分"><a href="#item-cf部分" class="headerlink" title="item-cf部分:"></a>item-cf部分:</h5><p>计算电影之间相似度的方法与上面计算用户之间相似度的方法类似，即相似度 = 评价过电影i和电影j的用户数 / √(评价过电影i的用户数 * 评价过电影j的用户数)，同样可以通过建立用户-物品倒排表来减少计算量，具体的实现代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def calc_movie_sim(self):</span><br><span class="line">	for user, movies in self.trainset.items():</span><br><span class="line">		for movie in movies:</span><br><span class="line">			# 计算每部电影评价的用户数</span><br><span class="line">			if movie not in self.movie_popular:</span><br><span class="line">				self.movie_popular[movie] = 0</span><br><span class="line">			self.movie_popular[movie] += 1</span><br><span class="line"></span><br><span class="line">	# 计算两两电影的共同评价用户数</span><br><span class="line">	itemsim_mat = self.movie_sim_mat</span><br><span class="line">	for user, movies in self.trainset.items():</span><br><span class="line">		for m1 in movies:</span><br><span class="line">			for m2 in movies:</span><br><span class="line">				if m1 == m2:</span><br><span class="line">					continue</span><br><span class="line">				itemsim_mat.setdefault(m1, &#123;&#125;)</span><br><span class="line">				itemsim_mat[m1].setdefault(m2, 0)</span><br><span class="line">				itemsim_mat[m1][m2] += 1</span><br><span class="line"></span><br><span class="line">	# 计算相似矩阵</span><br><span class="line">    for m1, related_movies in itemsim_mat.items():</span><br><span class="line">        for m2, count in related_movies.items():</span><br><span class="line">            itemsim_mat[m1][m2] = count / math.sqrt(</span><br><span class="line">                self.movie_popular[m1] * self.movie_popular[m2])</span><br></pre></td></tr></table></figure>
<p>找出与某用户看过的某部电影 i 相似度最大的K部电影<br>遍历这K部电影，如果该用户看过则跳过，否则则尝试将其加入候选推荐电影列表，如果已在列表中，则在原来的基础上将推荐指数加上相似度与电影 i 评分的乘积，否则则加入列表并将初始推荐指数设为相似度与电影 i 评分的乘积<br>遍历该用户看过的所有电影，最后可以得到一个推荐列表，返回此列表中推荐指数最高的N部电影</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def recommend(self, user):</span><br><span class="line">	K = self.n_sim_movie</span><br><span class="line">	N = self.n_rec_movie</span><br><span class="line">	rank = &#123;&#125;</span><br><span class="line">	watched_movies = self.trainset[user]</span><br><span class="line">	for movie, rating in watched_movies.items():</span><br><span class="line">	for related_movie, similarity_factor in sorted(self.movie_sim_mat[movie].items(),</span><br><span class="line">                                                   key=itemgetter(1), reverse=True)[:K]:</span><br><span class="line">		if related_movie in watched_movies:</span><br><span class="line">			continue</span><br><span class="line">		rank.setdefault(related_movie, 0)</span><br><span class="line">		rank[related_movie] += similarity_factor * rating</span><br><span class="line">	# 返回N部推荐的电影</span><br><span class="line">	return sorted(rank.items(), key=itemgetter(1), reverse=True)[:N]</span><br></pre></td></tr></table></figure>
<h4 id="结果评价"><a href="#结果评价" class="headerlink" title="结果评价"></a>结果评价</h4><p>item-cf：<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-31/74247547.jpg" alt=""><br>user-cf:<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-31/37311911.jpg" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mobyIsMe.github.io/2018/10/01/movielens/" data-id="cjxxhv3tt0005k2r0zy9nchy0" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-tableau实践" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/30/tableau实践/" class="article-date">
  <time datetime="2018-09-30T10:20:14.000Z" itemprop="datePublished">2018-09-30</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/30/tableau实践/">tableau实践</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="北京市空气质量可视化"><a href="#北京市空气质量可视化" class="headerlink" title="北京市空气质量可视化"></a>北京市空气质量可视化</h4><p>从网上download一份2016年全年北京市空气质量的数据，做了一个可视化的图表，看图：<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/9083045.jpg" alt=""><br>通过上图我们可以清晰的看到2016年全年北京市的空气质量情况。<br>先看一下原始数据：<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/2949683.jpg" alt=""><br>step1：导入数据：<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/16339061.jpg" alt=""><br>step2：分析下我们想要的最终效果，我们需要在维度上按照季度和月份、以及天分类，而度量则是我们最终需要看的数据：空气质量也就是AQI指数。</p>
<p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/94699674.jpg" alt=""><br>step3：于是我们做如下操作：</p>
<p>讲日期拖到列标签、行标签(拖两次)，将日期单位分别置为：日、季度、月，然后选择智能显示中的第二个图表类型<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/38976031.jpg" alt=""></p>
<p>step4：发现和我们最终想要的还有一些差距。我们希望使用颜色来表示不同的污染程度，将AQI指数拖动到颜色上。同时我们不希望AQI对放块儿的大小起作用，于是我们把形状上的AQI指数去掉<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/70228190.jpg" alt=""></p>
<p>step5：同时我们不希望AQI对放块儿的大小起作用，于是我们把形状上的AQI指数去掉：<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/70829302.jpg" alt=""></p>
<p>step6：这时候我们需要改变方块儿的颜色：</p>
<p>这里根据我们队污染程度区间的定义：0~50：优；50~100：良；100~150：轻度污染；…..等</p>
<p>我们需要对AQI空气指数进行分组：<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/18485269.jpg" alt=""></p>
<p>step7：分组完成<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/6927311.jpg" alt=""></p>
<p>step8：我们把AQI在颜色上的标记去掉，把刚刚的AQI分组拖到颜色标记上：<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/51890107.jpg" alt=""></p>
<p>step9：oh，好像差不多了，只是颜色还有点诡异，没关系，我们改一下颜色：<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/90931843.jpg" alt=""><br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/58337257.jpg" alt=""><br>step10：我们给每个组分配上不同的颜色，使其更好区分污染程度。然后点击工作表-复制-图像：<br><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/94516553.jpg" alt=""><br>然后完成</p>
<h4 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h4><p><a href="https://www.jianshu.com/p/d81ebe0f7240" target="_blank" rel="noopener">https://www.jianshu.com/p/d81ebe0f7240</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mobyIsMe.github.io/2018/09/30/tableau实践/" data-id="cjxxhv3t50001k2r0iurgeqdw" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-Tableau基本概念" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/09/27/Tableau基本概念/" class="article-date">
  <time datetime="2018-09-27T07:44:00.000Z" itemprop="datePublished">2018-09-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/09/27/Tableau基本概念/">Tableau基本概念</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>[toc]</p>
<h4 id="工作区初识："><a href="#工作区初识：" class="headerlink" title="工作区初识："></a>工作区初识：</h4><p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/22837265.jpg" alt=""></p>
<p><strong>维度和度量：</strong><br>度量：往往是一个数值字段，将其拖放到功能区时，tableau默认会进行聚合运算，同时，会生成一个相应的轴</p>
<p>维度：往往是一些分类、时间方面的定性字段，将其拖放到功能区时，tableau不会对其进行字段，而是对视图区进行分区，生成各区的标题。</p>
<p> <strong>离散和连续</strong><br>在tableau中，蓝色是离散字段，绿色是连续字段，离散字段在行列功能区总是在视图中显示为标题，而连续字段则在试图中显示为轴：</p>
<h4 id="工作流程："><a href="#工作流程：" class="headerlink" title="工作流程："></a><strong>工作流程：</strong></h4><p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/31341777.jpg" alt=""></p>
<ul>
<li><p>连接数据：</p>
<ul>
<li>file type: excel，text，json，access，pdf，statistical file等</li>
<li>database type：oracle，mysql，Tableau server等</li>
</ul>
</li>
<li>构建数据视图：<br>连接到数据源后，您将获得Tableau环境中可用的所有列和数据。您可以将它们分为维，度量和创建任何所需的层次结构。使用这些，您构建的视图传统上称为报告。 Tableau提供了轻松的拖放功能来构建视图。</li>
<li><p>增强视图<br>上面创建的视图需要进一步增强使用过滤器，聚合，轴标签，颜色和边框的格式。</p>
</li>
<li><p>创建工作表<br>我们创建不同的工作表，以便对相同的数据或不同的数据创建不同的视图。</p>
</li>
<li><p>创建和组织仪表板<br>仪表板包含多个链接它的工作表。因此，任何工作表中的操作都可以相应地更改仪表板中的结果。</p>
</li>
<li><p>创建故事<br>故事是一个工作表，其中包含一系列工作表或仪表板，它们一起工作以传达信息。您可以创建故事以显示事实如何连接，提供上下文，演示决策如何与结果相关，或者只是做出有说服力的案例。</p>
</li>
</ul>
<p><strong>支持的数据类型：</strong></p>
<ul>
<li>STRING：    任何零个或多个字符的序列。 它们用单引号括起来。 引号本身可以通过写两次来包含在字符串中。\’Hello\’<br>\’Quoted\’\’quote\’</li>
<li>NUMBER    这些是整数或浮点数。 建议在计算中使用浮点数的四舍五入。</li>
<li>BOOLEAN    它们是逻辑值：TRUE，FALSE</li>
<li>DATE &amp; DATETIME    Tableau以几乎所有格式识别日期。 但是，如果我们需要强制tableau将字符串识别为日期，那么我们在数据之前放一个＃符号。<pre><code>“02/01/2015&quot;
    “＃3 March 1982&quot;
</code></pre></li>
</ul>
<h4 id="数据源常见操作"><a href="#数据源常见操作" class="headerlink" title="数据源常见操作"></a>数据源常见操作</h4><ul>
<li>数据连接：left，right，inner，full outer</li>
<li>数据混合：在多个数据源中有要在单个视图中一起分析的相关数据时使用,数据混合中涉及的两个来源称为主数据源和辅助数据源。将在主数据源和辅助数据源之间创建左连接，其中所有数据行都来自辅助数据源的主数据行和匹配数据行。</li>
</ul>
<h4 id="tableau运算"><a href="#tableau运算" class="headerlink" title="tableau运算"></a>tableau运算</h4><p><strong>运算符类型</strong><br>Tableau有多个运算符用于创建计算字段和公式。</p>
<ul>
<li>常规运算符</li>
<li>算术运算符</li>
<li>比较运算符</li>
<li>逻辑运算符: AND,OR,NOT<br><strong>函数</strong></li>
<li>数字函数：CEILING(2.145)= 3，POWER(5,3)= 125，ROUND(3.14152,2)= 3.14</li>
<li>字符串函数：LEN(“Tableau”)= 7，LTRIM(“Tableau”)=“Tableau”（删除前面的空格），REPLACE(“GreenBlueGreen”，“Blue”，“Red”)=“GreenRedGreen”，UPPER(“Tableau”)=“TABLEAU”</li>
<li>日期函数：IFNULL([Sales]，0)= [Sales]，ISDATE(“11/05/98”)= TRUE ，MIN()</li>
<li>逻辑函数:</li>
<li>聚合函数: AVG(expression),COUNT(expression),MEDIAN(expression),STDEV(expression)(返回基于样本总体的给定表达式中所有值的统计标准偏差。)<br><strong>表计算</strong><br>这些是应用于整个表中的值的计算。例如，为了计算运行总计或运行平均值，我们需要对整个列应用单个计算方法。无法对某些选定的行执行此类计算。</li>
<li><p>步骤1<br>选择要应用表计算的度量，并将其拖动到列架。</p>
</li>
<li><p>第2步<br>右键单击度量，然后选择快速表计算选项。</p>
</li>
<li><p>步骤-3<br>选择要应用于度量的以下选项之一。</p>
<ul>
<li>Running Total</li>
<li>Difference</li>
<li>Percent Difference</li>
<li>Percent of Total</li>
<li>Rank</li>
<li>Percentile</li>
<li>Moving Average</li>
<li>Year to Date (YTD) Total</li>
<li>Compound Growth Rate</li>
<li>Year over Year Growth</li>
<li>Year to Date (YTD) Growth</li>
</ul>
</li>
</ul>
<p><strong>LOD表达式</strong><br>LOD（level of detail）表达式有三种主要类型。</p>
<ul>
<li>FIXED LOD 此表达式使用指定的维度计算值，而不引用视图中的任何其他维度。</li>
<li>INCLUDE LOD 此级别的详细信息表达式使用指定的维度以及视图中的任何维度来计算值。</li>
<li>EXCLUDE LOD 这些级别的细节表达式从视图细节级别中减去维度。</li>
</ul>
<p><strong>过滤器</strong><br>过滤是从结果集中删除某些值或值范围的过程。</p>
<ul>
<li>基本过滤器<ul>
<li>过滤器维度是应用于维度字段的过滤器。</li>
<li>过滤器度量是应用于度量字段的过滤器。</li>
<li>过滤器日期是应用于日期字段的过滤器。</li>
</ul>
</li>
<li>快速过滤器：<br>Tableau中的许多过滤器类型可以使用对维或度量的右键单击选项快速可用。 这些称为快速过滤器的过滤器具有足够的功能，可以解决大多数常见的过滤需求。</li>
<li>上下文过滤器：第二个过滤器只处理第一个过滤器返回的记录。因此，在这种情况下，第二个过滤器称为依赖过滤器，因为它们只处理通过上下文过滤器的数据。上下文过滤器有两个主要目的。<ul>
<li>提高性能 - 如果设置了大量过滤器或具有大型数据源，查询可能会很慢。您可以设置一个或多个上下文过滤器以提高性能。</li>
<li>创建从属数字或前N个过滤器 - 您可以设置上下文过滤器以仅包括感兴趣的数据，然后设置数字或前N个过滤器。</li>
</ul>
</li>
<li>创建过滤器：<ul>
<li>通用过滤器，允许从列表中选择特定值。</li>
<li>通配符过滤器允许提到通配符像cha *过滤所有以cha开头的字符串值。</li>
<li>条件过滤器，我用来应用条件，如销售额。</li>
<li>顶部过滤器选择代表Top值范围的记录。</li>
</ul>
</li>
</ul>
<h4 id="tableau图表："><a href="#tableau图表：" class="headerlink" title="tableau图表："></a>tableau图表：</h4><p><img src="http://phal1xgub.bkt.clouddn.com/18-10-28/91367091.jpg" alt=""></p>
<ul>
<li>柱状图：它的适用场合是二维数据集（每个数据点包括两个值x和y），但只有一个维度需要比较。</li>
<li>折线图（Line Chart）数据：折线图适合二维的大数据集，尤其是那些趋势比单个数据点更重要的场合。</li>
<li>饼图（Pie Chart）：饼图是一种应该避免使用的图表，因为肉眼对面积大小不敏感。</li>
<li>散点图：适用于三维数据集，但其中只有两维需要比较。</li>
<li>气泡图：是散点图的一种变体，通过每个点的面积大小，反映第三维。如果为气泡加上不同颜色（或文字标签），气泡图就可用来表达四维数据。比如下图就是通过颜色，表示每个点的风力等级。</li>
<li>雷达图：适用于多维数据（四维以上），且每个维度必须可以排序（国籍就不可以排序）。但是，它有一个局限，就是数据点最多6个，否则无法辨别，因此适用场合有限。</li>
</ul>
<h4 id="参考网站："><a href="#参考网站：" class="headerlink" title="参考网站："></a>参考网站：</h4><ul>
<li><p>蓝鲸的网站分析笔记：<a href="http://bluewhale.cc" target="_blank" rel="noopener">http://bluewhale.cc</a></p>
</li>
<li><p>网站分析在中国：<a href="http://www.chinawebanalytics.cn" target="_blank" rel="noopener">http://www.chinawebanalytics.cn</a></p>
</li>
<li>英文教程：<a href="https://www.guru99.com/tableau-charts-graphs-tutorial.html" target="_blank" rel="noopener">https://www.guru99.com/tableau-charts-graphs-tutorial.html</a></li>
<li>中文教程：<a href="https://www.w3cschool.cn/tableau/tableau_bar_chart.html" target="_blank" rel="noopener">https://www.w3cschool.cn/tableau/tableau_bar_chart.html</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://mobyIsMe.github.io/2018/09/27/Tableau基本概念/" data-id="cjxxhv3ta0003k2r0xq7u3txs" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据分析，数据挖掘/">数据分析，数据挖掘</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/数据分析，数据挖掘/" style="font-size: 10px;">数据分析，数据挖掘</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/07/10/一键生成数据报告/">一键生成数据报告</a>
          </li>
        
          <li>
            <a href="/2019/06/11/数据预处理常见姿势/">数据预处理常见姿势</a>
          </li>
        
          <li>
            <a href="/2019/05/10/特征选择/">特征选择</a>
          </li>
        
          <li>
            <a href="/2018/10/16/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2018/10/10/推荐架构/">推荐架构</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 moby<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>